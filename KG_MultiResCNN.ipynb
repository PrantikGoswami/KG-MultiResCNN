{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Checking GPU properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RrtEYeollPWz"
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important libraries to Install before start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-rCPJjq8pALX"
   },
   "outputs": [],
   "source": [
    "# !pip install overrides==3.1.0\n",
    "# !pip install allennlp==0.8.4\n",
    "# !pip install pytorch-pretrained-bert\n",
    "# !pip install transformers==4.4.2\n",
    "# !pip install entmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static arguments and other model hyper-parameters declaration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fHnawMktqM--"
   },
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['origin_path'] = '/content/drive/MyDrive/Thesis/DeepDifferentialDiagnosis/data/MIMIC-III/mimic-iii-clinical-database-1.4/'\n",
    "args['out_path'] = '/content/drive/MyDrive/Thesis/DeepDifferentialDiagnosis/data/25.10.2021-Old-Compare/test/'\n",
    "args['min_sentence_len'] = 3\n",
    "args['random_seed'] = 1\n",
    "args['vocab'] = '%s%s.csv' % (args['out_path'], 'vocab')\n",
    "args['vocab_min'] = 3\n",
    "args['Y'] = 'full' #'50'/'full'\n",
    "args['data_path'] = '%s%s_%s.csv' % (args['out_path'], 'train', args['Y']) #50/full\n",
    "args['version'] = 'mimic3'\n",
    "args['model'] = 'KG_MultiResCNN' #'KG_MultiResCNNLSTM','bert_we_kg'\n",
    "args['gpu'] = 0\n",
    "args['embed_file'] = '%s%s_%s.embed' % (args['out_path'], 'processed', 'full')\n",
    "args['use_ext_emb'] = False\n",
    "args['dropout'] = 0.2\n",
    "args['num_filter_maps'] = 50\n",
    "args['conv_layer'] = 2\n",
    "args['filter_size'] = '3,5,7,9,13,15,17,23,29' #'3,5,9,15,19,25', '3,5,7,9,13,15,17,23,29'\n",
    "args['test_model'] = None\n",
    "args['weight_decay'] = 0 #Adam, 0.01 #AdamW\n",
    "args['lr'] = 0.001 #0.0005, 0.001, 0.00146 best 3 for adam and Adamw, 1e-5 for Bert\n",
    "args['tune_wordemb'] = True\n",
    "args['MAX_LENGTH'] = 3000 #2500, 512 for bert #1878 is the avg length and max length is 10504 for only discharge summary. 238438 is the max length, 3056 is the avg length combined DS+PHY+NUR\n",
    "args['batch_size'] = 6 #8,16\n",
    "args['n_epochs'] = 15\n",
    "args['MODEL_DIR'] = '/content/drive/MyDrive/Thesis/DeepDifferentialDiagnosis/data/model_output'\n",
    "args['criterion'] = 'prec_at_8'\n",
    "args['for_test'] = False\n",
    "args['bert_dir'] = '/content/drive/MyDrive/Thesis/DeepDifferentialDiagnosis/data/Bert/'\n",
    "args['pretrained_bert'] = 'bert-base-uncased' # 'emilyalsentzer/Bio_ClinicalBERT''bert-base-uncased' 'dmis-lab/biobert-base-cased-v1.1'\n",
    "args['instance_count'] = 'full' #if not full then the number specified here will be the number of samples.\n",
    "args['graph_embedding_file'] = '/home/pgoswami/DifferentialEHR/data/Pytorch-BigGraph/wikidata_translation_v1.tsv.gz'\n",
    "args['entity_dimention'] = 200 #pytorch biggraph entity has dimention size of 200\n",
    "# args['entity_selected'] = 5\n",
    "args['MAX_ENT_LENGTH'] = 30 #mean value is 27.33, for DS+PY+NR max 49, avg 29\n",
    "args['use_embd_layer'] = True\n",
    "args['add_with_wordrap'] = True\n",
    "args['step_size'] = 8\n",
    "args['gamma'] = 0.1\n",
    "args['patience'] = 10 #if does not improve result for 5 epochs then break.\n",
    "args['use_schedular'] = True\n",
    "args['grad_clip'] = False\n",
    "args['use_entmax15'] = False\n",
    "args['use_sentiment']=False\n",
    "args['sentiment_bert'] = 'siebert/sentiment-roberta-large-english'\n",
    "args['use_tfIdf'] = True\n",
    "args['use_proc_label'] = True\n",
    "args['notes_type'] = 'Discharge summary' # 'Discharge summary,Nursing,Physician ' / 'Nursing,Physician '\n",
    "args['comment'] = \"\"\" My changes with 3000 token+30 ent embed. Tf-idf weight. Diag_ICD+Prod_ICD used, 50 codes \"\"\"\n",
    "args['save_everything'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wikidump creation process and indexing\n",
    "# import wikimapper\n",
    "# wikimapper.download_wikidumps(dumpname=\"enwiki-latest\", path=\"/home/pgoswami/DifferentialEHR/data/Wikidata_dump/\")\n",
    "# wikimapper.create_index(dumpname=\"enwiki-latest\",path_to_dumps=\"/home/pgoswami/DifferentialEHR/data/Wikidata_dump/\", \n",
    "#                         path_to_db= \"/home/pgoswami/DifferentialEHR/data/Wikidata_dump/index_enwiki-latest.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Biggraph pre-trained embedding file downloaded from \n",
    "#https://github.com/facebookresearch/PyTorch-BigGraph#pre-trained-embeddings\n",
    "# to '/home/pgoswami/DifferentialEHR/data/Pytorch-BigGraph/wikidata_translation_v1.tsv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessedIter(object):\n",
    "\n",
    "    def __init__(self, Y, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename) as f:\n",
    "            r = csv.reader(f)\n",
    "            next(r)\n",
    "            for row in r:\n",
    "                yield (row[2].split()) #after group-by with subj_id and hadm_id, text is in 3rd column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import csv\n",
    "import sys\n",
    "import operator\n",
    "# import operator\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "import gensim.models\n",
    "import gensim.models.word2vec as w2v\n",
    "import gensim.models.fasttext as fasttext\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nlp_tool = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "import re\n",
    "from transformers import pipeline #for entity extraction\n",
    "from wikimapper import WikiMapper #creating wikidata entity id\n",
    "\n",
    "import pickle\n",
    "import smart_open as smart\n",
    "\n",
    "class DataProcessing:\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        # step 1: process code-related files\n",
    "        dfdiag = pd.read_csv(args['origin_path']+'DIAGNOSES_ICD.csv')\n",
    "        if args['use_proc_label']:\n",
    "            dfproc = pd.read_csv(args['origin_path']+'PROCEDURES_ICD.csv')\n",
    "        \n",
    "        dfdiag['absolute_code'] = dfdiag.apply(lambda row: str(self.reformat(str(row[4]), True)), axis=1)\n",
    "        if args['use_proc_label']:\n",
    "            dfproc['absolute_code'] = dfproc.apply(lambda row: str(self.reformat(str(row[4]), False)), axis=1)\n",
    "        \n",
    "        dfcodes = pd.concat([dfdiag, dfproc]) if args['use_proc_label'] else dfdiag\n",
    "        \n",
    "        dfcodes.to_csv(args['out_path']+'ALL_CODES.csv', index=False,\n",
    "           columns=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'absolute_code'],\n",
    "           header=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE']) #columns: 'ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE'\n",
    "        print(\"unique ICD9 code: {}\".format(len(dfcodes['absolute_code'].unique())))\n",
    "        \n",
    "        del dfcodes\n",
    "        if args['use_proc_label']:\n",
    "            del dfproc\n",
    "        del dfdiag\n",
    "        \n",
    "        # step 2: process notes\n",
    "        # min_sentence_len = 3\n",
    "        disch_full_file = self.write_discharge_summaries(args['out_path']+'disch_full_acc.csv', args['min_sentence_len'], args['origin_path']+'NOTEEVENTS.csv')\n",
    "        dfnotes = pd.read_csv(args['out_path']+'disch_full_acc.csv')\n",
    "        dfnotes = dfnotes.sort_values(['SUBJECT_ID', 'HADM_ID'])\n",
    "        dfnotes = dfnotes.drop_duplicates()\n",
    "        dfnotes = dfnotes.groupby(['SUBJECT_ID','HADM_ID']).apply(lambda x: pd.Series({'TEXT':' '.join(str(v) for v in x.TEXT)})).reset_index()\n",
    "        dfnotes.to_csv(args['out_path']+'disch_full.csv', index=False) #columns: 'SUBJECT_ID', 'HADM_ID', 'TEXT'\n",
    "        \n",
    "        # step 3: filter out the codes that not emerge in notes\n",
    "        subj_ids = set(dfnotes['SUBJECT_ID'])\n",
    "        self.code_filter(args['out_path'], subj_ids) \n",
    "        dfcodes_filtered = pd.read_csv(args['out_path']+'ALL_CODES_filtered_acc.csv', index_col=None)\n",
    "        dfcodes_filtered = dfcodes_filtered.sort_values(['SUBJECT_ID', 'HADM_ID'])\n",
    "        dfcodes_filtered.to_csv(args['out_path']+'ALL_CODES_filtered.csv', index=False) #columns: 'SUBJECT_ID', 'HADM_ID', 'ICD9_CODE', 'ADMITTIME', 'DISCHTIME'\n",
    "        del dfnotes\n",
    "        del dfcodes_filtered\n",
    "        \n",
    "        # step 4: link notes with their code\n",
    "#         labeled = self.concat_data(args['out_path']+'ALL_CODES_filtered.csv', args['out_path']+'disch_full.csv', args['out_path']+'notes_labeled.csv')\n",
    "        labeled = self.concat_data_new(args['out_path']+'ALL_CODES_filtered.csv', args['out_path']+'disch_full.csv', args['out_path']+'notes_labeled.csv')\n",
    "         #columns: 'SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS'\n",
    "        \n",
    "        labled_notes = pd.read_csv(labeled) #columns: 'SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS'\n",
    "\n",
    "        labled_notes = labled_notes.drop_duplicates()\n",
    "        labled_notes.to_csv(labeled, index=False) #columns: 'SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS'\n",
    "        \n",
    "        # step 5: statistic unique word, total word, HADM_ID number\n",
    "        types = set()\n",
    "        num_tok = 0\n",
    "        for row in labled_notes.itertuples(): \n",
    "            for w in row[3].split(): #TEXT in 4rd column when used itertuples\n",
    "                types.add(w)\n",
    "                num_tok += 1\n",
    "        \n",
    "        print(\"num types\", len(types), \"num tokens\", num_tok)\n",
    "        print(\"HADM_ID: {}\".format(len(labled_notes['HADM_ID'].unique())))\n",
    "        print(\"SUBJECT_ID: {}\".format(len(labled_notes['SUBJECT_ID'].unique())))\n",
    "        del labled_notes\n",
    "        \n",
    "        \n",
    "        \n",
    "        #important step for entity extraction and finding their entity id from wikidata.\n",
    "        fname_entity = self.extract_entity('%snotes_labeled.csv' % args['out_path'],  '%snotes_labeled_entity.csv' % args['out_path'])\n",
    "        #columns: 'SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS', 'ENTITY_ID'\n",
    "        \n",
    "        #important step to create embedding file from Pytorch Biggraph pretrained embedding file for our dataset entities.\n",
    "        self.extract_biggraph_embedding(fname_entity, args['graph_embedding_file'], '%sentity2embedding.pickle' % args['out_path'])\n",
    "        \n",
    "        \n",
    "        # step 6: split data into train dev test\n",
    "        # step 7: sort data by its note length, add length to the last column\n",
    "        \n",
    "        tr, dv, te = self.split_length_sort_data(fname_entity, args['out_path'], 'full') #full data split and save\n",
    "        #columns: 'SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS', 'ENTITY_ID', 'length'\n",
    "        \n",
    "        # vocab_min = 3\n",
    "        vname = '%svocab.csv' % args['out_path']\n",
    "        self.build_vocab(args['vocab_min'], tr, vname)\n",
    "        \n",
    "        \n",
    "        # step 8: train word embeddings via word2vec and fasttext\n",
    "        Y = 'full'\n",
    "        #if want to create vocabulary from w2v model then pass the vocabulary file name where you want to save the vocabulary\n",
    "        w2v_file = self.word_embeddings('full', '%sdisch_full.csv' % args['out_path'], 100, 0, 5)\n",
    "        self.gensim_to_embeddings('%sprocessed_full.w2v' % args['out_path'], '%svocab.csv' % args['out_path'], Y)\n",
    "        self.fasttext_file = self.fasttext_embeddings('full', '%sdisch_full.csv' % args['out_path'], 100, 0, 5)\n",
    "        self.gensim_to_fasttext_embeddings('%sprocessed_full.fasttext' % args['out_path'], '%svocab.csv' % args['out_path'], Y)\n",
    "        \n",
    "        # step 9: statistic the top 50 code\n",
    "        Y = 50\n",
    "        counts = Counter()\n",
    "        dfnl = pd.read_csv(fname_entity) \n",
    "        for row in dfnl.itertuples(): #for read_csv and iteratuples, the first column (row[0]) is the index column\n",
    "            for label in str(row[4]).split(';'): #lables are in 4th position\n",
    "                counts[label] += 1\n",
    "\n",
    "        codes_50 = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        codes_50 = [code[0] for code in codes_50[:Y]]\n",
    "\n",
    "        with open('%sTOP_%s_CODES.csv' % (args['out_path'], str(Y)), 'w') as of:\n",
    "            w = csv.writer(of)\n",
    "            for code in codes_50:\n",
    "                w.writerow([code])\n",
    "        \n",
    "        with open(fname_entity, 'r') as f: #columns: 'SUBJECT_ID', 'TEXT', 'LABELS', 'ENTITY_ID'\n",
    "            with open('%snotes_labeled_50.csv' % args['out_path'], 'w') as fl:\n",
    "                r = csv.reader(f)\n",
    "                w = csv.writer(fl)\n",
    "                #header\n",
    "                w.writerow(next(r))\n",
    "                newrow = False\n",
    "                for row in r:\n",
    "                    newrow = True\n",
    "                    for code in codes_50:\n",
    "                        if code in str(row[3]).split(';'):\n",
    "                            if newrow:\n",
    "                                w.writerow(row)\n",
    "                                newrow = False\n",
    "        \n",
    "        fname_50 = '%snotes_labeled_50.csv' % args['out_path'] #input dataframe\n",
    "                \n",
    "        tr, dv, te = self.split_length_sort_data(fname_50, args['out_path'], str(Y))\n",
    "        #columns: 'SUBJECT_ID', 'TEXT', 'LABELS', 'ENTITY_ID', 'length'\n",
    "        \n",
    "        \n",
    "    def reformat(self, code, is_diag):\n",
    "        \"\"\"\n",
    "            Put a period in the right place because the MIMIC-3 data files exclude them.\n",
    "            Generally, procedure codes have dots after the first two digits,\n",
    "            while diagnosis codes have dots after the first three digits.\n",
    "        \"\"\"\n",
    "        code = ''.join(code.split('.'))\n",
    "        if is_diag:\n",
    "            if code.startswith('E'):\n",
    "                if len(code) > 4:\n",
    "                    code = code[:4] + '.' + code[4:]\n",
    "            else:\n",
    "                if len(code) > 3:\n",
    "                    code = code[:3] + '.' + code[3:]\n",
    "        else:\n",
    "            code = code[:2] + '.' + code[2:]\n",
    "        return code\n",
    "    \n",
    "    def write_discharge_summaries(self, out_file, min_sentence_len, notes_file):\n",
    "        print(\"processing notes file\")\n",
    "        with open(notes_file, 'r') as csvfile:\n",
    "            with open(out_file, 'w') as outfile:\n",
    "                print(\"writing to %s\" % (out_file))\n",
    "                outfile.write(','.join(['SUBJECT_ID', 'HADM_ID', 'CHARTTIME', 'TEXT']) + '\\n')\n",
    "                notereader = csv.reader(csvfile)\n",
    "                next(notereader)\n",
    "\n",
    "                for line in tqdm(notereader):\n",
    "                    subj = int(float(line[1]))\n",
    "                    category = line[6]\n",
    "                    if category in  args['notes_type'].split(','): #can Includes \"Nursing\" and \"Physician\".\n",
    "                        note = line[10]\n",
    "\n",
    "                        all_sents_inds = []\n",
    "                        generator = nlp_tool.span_tokenize(note)\n",
    "                        for t in generator:\n",
    "                            all_sents_inds.append(t)\n",
    "\n",
    "                        text = \"\"\n",
    "                        for ind in range(len(all_sents_inds)):\n",
    "                            start = all_sents_inds[ind][0]\n",
    "                            end = all_sents_inds[ind][1]\n",
    "\n",
    "                            sentence_txt = note[start:end] \n",
    "                            \n",
    "                            sentence_txt = re.sub(r'[[**].+?[**]]', '', sentence_txt) #adding to remove texts between [** **]\n",
    "\n",
    "                            tokens = [t.lower() for t in tokenizer.tokenize(sentence_txt) if not t.isnumeric()]\n",
    "                            if ind == 0:\n",
    "                                text += '[CLS] ' + ' '.join(tokens) + ' [SEP]'\n",
    "                            else:\n",
    "                                text += ' [CLS] ' + ' '.join(tokens) + ' [SEP]'\n",
    "\n",
    "                        text = '\"' + text + '\"'\n",
    "                        outfile.write(','.join([line[1], line[2], line[4], text]) + '\\n')\n",
    "\n",
    "\n",
    "        return out_file\n",
    "    \n",
    "    def code_filter(self, out_path, subj_ids):\n",
    "        with open(out_path+'ALL_CODES.csv', 'r') as lf:\n",
    "            with open(out_path+'ALL_CODES_filtered_acc.csv', 'w') as of:\n",
    "                w = csv.writer(of)\n",
    "                w.writerow(['SUBJECT_ID', 'HADM_ID', 'ICD9_CODE', 'ADMITTIME', 'DISCHTIME'])\n",
    "                r = csv.reader(lf)\n",
    "                #header\n",
    "                next(r)\n",
    "                for i,row in enumerate(r):\n",
    "                    subj_id = int(float(row[1]))\n",
    "                    if subj_id in subj_ids:\n",
    "                        w.writerow(row[1:3] + [row[-1], '', ''])\n",
    "                        \n",
    "                        \n",
    "    def concat_data_new(self, labelsfile, notes_file, outfilename):\n",
    "        \n",
    "        print(\"labelsfile=\",labelsfile) #columns: 'SUBJECT_ID', 'HADM_ID', 'ICD9_CODE', 'ADMITTIME', 'DISCHTIME'\n",
    "        print(\"notes_file=\",notes_file) #columns: 'SUBJECT_ID', 'HADM_ID', 'TEXT'\n",
    "        \n",
    "        mydf_label = pd.read_csv(labelsfile)\n",
    "        mydf_label = mydf_label.groupby(['SUBJECT_ID','HADM_ID']).apply(lambda x: pd.Series({'ICD9_CODE':';'.join(str(v) for v in x.ICD9_CODE)})).reset_index()\n",
    "        \n",
    "        mydf_notes = pd.read_csv(notes_file) #already groupby with [subj,hadm]\n",
    "        \n",
    "        merged_df = pd.merge(mydf_notes, mydf_label, how='inner', on=['SUBJECT_ID','HADM_ID']).rename(columns={\"ICD9_CODE\": \"LABELS\"})\n",
    "        merged_df.to_csv(outfilename, index=False)\n",
    "        \n",
    "        del merged_df\n",
    "        return outfilename\n",
    "        \n",
    "        \n",
    "    #used in old data process.     \n",
    "    def concat_data(self, labelsfile, notes_file, outfilename):\n",
    "        \"\"\"\n",
    "            INPUTS:\n",
    "                labelsfile: sorted by hadm id, contains one label per line\n",
    "                notes_file: sorted by hadm id, contains one note per line\n",
    "        \"\"\"\n",
    "        csv.field_size_limit(sys.maxsize)\n",
    "        print(\"labelsfile=\",labelsfile) #columns: 'SUBJECT_ID', 'HADM_ID', 'ICD9_CODE', 'ADMITTIME', 'DISCHTIME'\n",
    "        print(\"notes_file=\",notes_file) #columns: 'SUBJECT_ID', 'HADM_ID', 'TEXT'\n",
    "        with open(labelsfile, 'r') as lf:\n",
    "            print(\"CONCATENATING\")\n",
    "            with open(notes_file, 'r') as notesfile:\n",
    "\n",
    "                with open(outfilename, 'w') as outfile:\n",
    "                    w = csv.writer(outfile)\n",
    "                    w.writerow(['SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS'])\n",
    "\n",
    "                    labels_gen = self.next_labels(lf)\n",
    "                    notes_gen = self.next_notes(notesfile)\n",
    "\n",
    "                    for i, (subj_id, text, hadm_id) in enumerate(notes_gen):\n",
    "                        if i % 10000 == 0:\n",
    "                            print(str(i) + \" done\")\n",
    "                        cur_subj, cur_labels, cur_hadm = next(labels_gen)\n",
    "                        \n",
    "                        if cur_hadm == hadm_id:\n",
    "                            w.writerow([subj_id, str(hadm_id), text, ';'.join(cur_labels)])\n",
    "                        else:\n",
    "                            print(\"couldn't find matching hadm_id. data is probably not sorted correctly\")\n",
    "                            break\n",
    "\n",
    "        return outfilename\n",
    "    \n",
    "    def next_labels(self, labelsfile): #columns: 'SUBJECT_ID', 'HADM_ID', 'ICD9_CODE', 'ADMITTIME', 'DISCHTIME'\n",
    "        \"\"\"\n",
    "            Generator for label sets from the label file\n",
    "        \"\"\"\n",
    "        labels_reader = csv.reader(labelsfile)\n",
    "        # header\n",
    "        next(labels_reader)\n",
    "\n",
    "        first_label_line = next(labels_reader)\n",
    "\n",
    "        cur_subj = int(first_label_line[0])\n",
    "        cur_hadm = int(first_label_line[1])\n",
    "        cur_labels = [first_label_line[2]]\n",
    "\n",
    "        for row in labels_reader:\n",
    "            subj_id = int(row[0])\n",
    "            hadm_id = int(row[1])\n",
    "            code = row[2]\n",
    "            # keep reading until you hit a new hadm id\n",
    "            if hadm_id != cur_hadm or subj_id != cur_subj:\n",
    "                yield cur_subj, cur_labels, cur_hadm\n",
    "                cur_labels = [code]\n",
    "                cur_subj = subj_id\n",
    "                cur_hadm = hadm_id\n",
    "            else:\n",
    "                # add to the labels and move on\n",
    "                cur_labels.append(code)\n",
    "        yield cur_subj, cur_labels, cur_hadm\n",
    "        \n",
    "    def next_notes(self, notesfile): #columns: 'SUBJECT_ID', 'HADM_ID', 'TEXT'\n",
    "        \"\"\"\n",
    "            Generator for notes from the notes file\n",
    "            This will also concatenate discharge summaries and their addenda, which have the same subject and hadm id\n",
    "        \"\"\"\n",
    "        nr = csv.reader(notesfile)\n",
    "        # header\n",
    "        next(nr)\n",
    "\n",
    "        first_note = next(nr)\n",
    "\n",
    "        cur_subj = int(first_note[0])\n",
    "        cur_hadm = int(first_note[1])\n",
    "        cur_text = first_note[2] \n",
    "\n",
    "        for row in nr:\n",
    "            subj_id = int(row[0])\n",
    "            hadm_id = int(row[1])\n",
    "            text = row[2] \n",
    "            # keep reading until you hit a new hadm id\n",
    "            if hadm_id != cur_hadm or subj_id != cur_subj:\n",
    "                yield cur_subj, cur_text, cur_hadm\n",
    "                cur_text = text\n",
    "                cur_subj = subj_id\n",
    "                cur_hadm = hadm_id\n",
    "            else:\n",
    "                # concatenate to the discharge summary and move on\n",
    "                cur_text += \" \" + text\n",
    "        yield cur_subj, cur_text, cur_hadm\n",
    "        \n",
    "    def extract_entity(self, data_file, out_file): #data file columns: 'SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS'\n",
    "        #Pre-trained Entity extraction model from Huggingface\n",
    "        unmasker = pipeline('ner', model='samrawal/bert-base-uncased_clinical-ner')\n",
    "        #wikimapper from downloaded and indexed wikidump\n",
    "        mapper = WikiMapper(\"/home/pgoswami/DifferentialEHR/data/Wikidata_dump/index_enwiki-latest.db\")\n",
    "        \n",
    "        csv.field_size_limit(sys.maxsize)\n",
    "        \n",
    "        with open(data_file, 'r') as lf:\n",
    "            with open(out_file, 'w') as of:\n",
    "                w = csv.writer(of)\n",
    "                w.writerow(['SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS', 'ENTITY_ID'])\n",
    "                r = csv.reader(lf)\n",
    "                #header\n",
    "                next(r)\n",
    "                for i,row in enumerate(r):\n",
    "                    if i % 1000 == 0:\n",
    "                        print(str(i) + \" entity extraction done\")\n",
    "                            \n",
    "                    text = str(row[2])\n",
    "                    extracted_entities = ' '.join([x for x in [obj['word'] for obj in unmasker(text)[0:50]]])\n",
    "                    fine_text = extracted_entities.replace(' ##', '').split()\n",
    "                    entity_ids = ' '.join([mapper.title_to_id(m.title()) for m in fine_text if mapper.title_to_id(m.title()) is not None]) #getting the title ids from wikidata\n",
    "                    w.writerow(row + [entity_ids])\n",
    "        return out_file\n",
    "    \n",
    "    def extract_biggraph_embedding(self, data_file, embedding_file_path, out_file): \n",
    "        #datafile columns :'SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS', 'ENTITY_ID\n",
    "        selected_entity_ids = set()\n",
    "        \n",
    "        with open(data_file, 'r') as lf:\n",
    "            r = csv.reader(lf)\n",
    "            #header\n",
    "            next(r)\n",
    "            for i,row in enumerate(r):\n",
    "                entity_ids = str(row[4]).split()\n",
    "                selected_entity_ids.update(entity_ids)\n",
    "        \n",
    "        print(f'Total {len(selected_entity_ids)} QIDs for Entities')\n",
    "        \n",
    "        entity2embedding = {}\n",
    "        \n",
    "        with smart.open(embedding_file_path, encoding='utf-8') as fp:  # smart open can read .gz files\n",
    "            for i, line in enumerate(fp):\n",
    "                cols = line.split('\\t')\n",
    "\n",
    "                entity_id = cols[0]\n",
    "\n",
    "                if entity_id.startswith('<http://www.wikidata.org/entity/Q') and entity_id.endswith('>'):\n",
    "                    entity_id = entity_id.replace('<http://www.wikidata.org/entity/', '').replace('>', '')\n",
    "\n",
    "                    if entity_id in selected_entity_ids:\n",
    "                        entity2embedding[entity_id] = np.array(cols[1:]).astype(np.float)\n",
    "\n",
    "                if not i % 100000:\n",
    "                    print(f'Lines completed {i}')\n",
    "\n",
    "        # Save\n",
    "        with open(out_file, 'wb') as f:\n",
    "            pickle.dump(entity2embedding, f)\n",
    "\n",
    "        print(f'Embeddings Saved to {out_file}')\n",
    "\n",
    "\n",
    "    #datasetType = full/50, \n",
    "    #labeledfile=inputfilepath, \n",
    "    #base_name=outputfilename\n",
    "    def split_length_sort_data(self, labeledfile, base_name, datsetType): \n",
    "        print(\"SPLITTING\")\n",
    "        labeledDf = pd.read_csv(labeledfile)\n",
    "        labeledDf['length'] = labeledDf.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
    "\n",
    "        labeledDf_train = labeledDf.sample(frac = 0.7) #70% train data\n",
    "        labeledDf_remain = labeledDf.drop(labeledDf_train.index)\n",
    "        labeledDf_dev = labeledDf_remain.sample(frac = 0.5) #15% val data\n",
    "        labeledDf_test = labeledDf_remain.drop(labeledDf_dev.index) #15% test data\n",
    "        \n",
    "        filename_list = []\n",
    "        for splt in ['train', 'dev', 'test']:\n",
    "            filename = '%s%s_full.csv' % (base_name, splt) if datsetType == 'full' else '%s%s_%s.csv' % (base_name, splt, '50')\n",
    "            conv_df = eval('labeledDf_'+splt) #getting the variable\n",
    "            conv_df = conv_df.sort_values(['length'])\n",
    "            print('saving to ..'+filename)\n",
    "            filename_list.append(filename)\n",
    "            conv_df.to_csv(filename, index=False)\n",
    "        \n",
    "        #gc the dataframes\n",
    "        del labeledDf_train\n",
    "        del labeledDf_remain\n",
    "        del labeledDf_dev\n",
    "        del labeledDf_test\n",
    "        \n",
    "        return filename_list[0], filename_list[1], filename_list[2]\n",
    "    \n",
    "    def build_vocab(self, vocab_min, infile, vocab_filename): #columns : 'SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS', 'ENTITY_ID', 'length'\n",
    "        \"\"\"\n",
    "            INPUTS:\n",
    "                vocab_min: how many documents a word must appear in to be kept\n",
    "                infile: (training) data file to build vocabulary from. CSV reader also need huge memory to load the file.\n",
    "                vocab_filename: name for the file to output\n",
    "        \"\"\"\n",
    "        csv.field_size_limit(sys.maxsize)\n",
    "        with open(infile, 'r') as csvfile: #columns: 'SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS', 'ENTITY_ID', 'length'\n",
    "            reader = csv.reader(csvfile)\n",
    "            # header\n",
    "            next(reader)\n",
    "\n",
    "            # 0. read in data\n",
    "            print(\"reading in data...\")\n",
    "            # holds number of terms in each document\n",
    "            note_numwords = []\n",
    "            # indices where notes start\n",
    "            note_inds = [0]\n",
    "            # indices of discovered words\n",
    "            indices = []\n",
    "            # holds a bunch of ones\n",
    "            data = []\n",
    "            # keep track of discovered words\n",
    "            vocab = {}\n",
    "            # build lookup table for terms\n",
    "            num2term = {}\n",
    "            # preallocate array to hold number of notes each term appears in\n",
    "            note_occur = np.zeros(400000, dtype=int)\n",
    "            i = 0\n",
    "            for row in reader:\n",
    "                text = row[2] #chnage Prantik: after merging same subject values, text is in third (2) position\n",
    "                numwords = 0\n",
    "                for term in text.split():\n",
    "                    # put term in vocab if it's not there. else, get the index\n",
    "                    index = vocab.setdefault(term, len(vocab))\n",
    "                    indices.append(index)\n",
    "                    num2term[index] = term\n",
    "                    data.append(1)\n",
    "                    numwords += 1\n",
    "                # record where the next note starts\n",
    "                note_inds.append(len(indices))\n",
    "                indset = set(indices[note_inds[-2]:note_inds[-1]])\n",
    "                # go thru all the word indices you just added, and add to the note occurrence count for each of them\n",
    "                for ind in indset:\n",
    "                    note_occur[ind] += 1\n",
    "                note_numwords.append(numwords)\n",
    "                i += 1\n",
    "            # clip trailing zeros\n",
    "            note_occur = note_occur[note_occur > 0]\n",
    "\n",
    "            # turn vocab into a list so indexing doesn't get fd up when we drop rows\n",
    "            vocab_list = np.array([word for word, ind in sorted(vocab.items(), key=operator.itemgetter(1))])\n",
    "\n",
    "            # 1. create sparse document matrix\n",
    "            C = csr_matrix((data, indices, note_inds), dtype=int).transpose()\n",
    "            # also need the numwords array to be a sparse matrix\n",
    "            note_numwords = csr_matrix(1. / np.array(note_numwords))\n",
    "\n",
    "            # 2. remove rows with less than 3 total occurrences\n",
    "            print(\"removing rare terms\")\n",
    "            # inds holds indices of rows corresponding to terms that occur in < 3 documents\n",
    "            inds = np.nonzero(note_occur >= vocab_min)[0]\n",
    "            print(str(len(inds)) + \" terms qualify out of \" + str(C.shape[0]) + \" total\")\n",
    "            # drop those rows\n",
    "            C = C[inds, :]\n",
    "            note_occur = note_occur[inds]\n",
    "            vocab_list = vocab_list[inds]\n",
    "\n",
    "            print(\"writing output\")\n",
    "            with open(vocab_filename, 'w') as vocab_file:\n",
    "                for word in vocab_list:\n",
    "                    vocab_file.write(word + \"\\n\")\n",
    "                    \n",
    "    def word_embeddings(self, Y, notes_file, embedding_size, min_count, n_iter, outfile=None):\n",
    "        modelname = \"processed_%s.w2v\" % (Y)\n",
    "        sentences = ProcessedIter(Y, notes_file)\n",
    "        print(\"Model name %s...\" % (modelname))\n",
    "\n",
    "        model = w2v.Word2Vec(vector_size=embedding_size, min_count=min_count, workers=4, epochs=n_iter)\n",
    "        print(\"building word2vec vocab on %s...\" % (notes_file))\n",
    "\n",
    "        model.build_vocab(sentences)\n",
    "        print(\"training...\")\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        out_file = '/'.join(notes_file.split('/')[:-1] + [modelname])\n",
    "        print(\"writing embeddings to %s\" % (out_file))\n",
    "        model.save(out_file)\n",
    "        \n",
    "        #if want to create vocab from w2v model, pass the vocab file name\n",
    "        if outfile is not None:\n",
    "            print(\"writing vocab to %s\" % (outfile))\n",
    "            with open(outfile, 'w') as vocab_file:\n",
    "                for word in  model.wv.key_to_index:\n",
    "                    vocab_file.write(word + \"\\n\")\n",
    "                \n",
    "        return out_file\n",
    "    \n",
    "    def gensim_to_embeddings(self, wv_file, vocab_file, Y, outfile=None):\n",
    "        model = gensim.models.Word2Vec.load(wv_file)\n",
    "        wv = model.wv\n",
    "        #free up memory\n",
    "        del model\n",
    "\n",
    "        vocab = set()\n",
    "        with open(vocab_file, 'r') as vocabfile:\n",
    "            for i,line in enumerate(vocabfile):\n",
    "                line = line.strip()\n",
    "                if line != '':\n",
    "                    vocab.add(line)\n",
    "        ind2w = {i+1:w for i,w in enumerate(sorted(vocab))}\n",
    "\n",
    "        W, words = self.build_matrix(ind2w, wv)\n",
    "\n",
    "        if outfile is None:\n",
    "            outfile = wv_file.replace('.w2v', '.embed')\n",
    "\n",
    "        #smash that save button\n",
    "        self.save_embeddings(W, words, outfile)\n",
    "        \n",
    "    def build_matrix(self, ind2w, wv):\n",
    "        \"\"\"\n",
    "            Go through vocab in order. Find vocab word in wv.index2word, then call wv.word_vec(wv.index2word[i]).\n",
    "            Put results into one big matrix.\n",
    "            Note: ind2w starts at 1 (saving 0 for the pad character), but gensim word vectors starts at 0\n",
    "        \"\"\"\n",
    "        W = np.zeros((len(ind2w)+1, len(wv.get_vector(wv.index_to_key[0])) ))\n",
    "        print(\"W shape=\",W.shape)\n",
    "        words = [\"**PAD**\"]\n",
    "        W[0][:] = np.zeros(len(wv.get_vector(wv.index_to_key[0])))\n",
    "        for idx, word in tqdm(ind2w.items()):\n",
    "            if idx >= W.shape[0]:\n",
    "                break\n",
    "            W[idx][:] = wv.get_vector(word)\n",
    "            words.append(word)\n",
    "        print(\"W shape final=\",W.shape)\n",
    "        print(\"Word list length=\",len(words))\n",
    "        return W, words\n",
    "    \n",
    "    def fasttext_embeddings(self, Y, notes_file, embedding_size, min_count, n_iter):\n",
    "        modelname = \"processed_%s.fasttext\" % (Y)\n",
    "        sentences = ProcessedIter(Y, notes_file)\n",
    "        print(\"Model name %s...\" % (modelname))\n",
    "\n",
    "        model = fasttext.FastText(vector_size=embedding_size, min_count=min_count, epochs=n_iter)\n",
    "        print(\"building fasttext vocab on %s...\" % (notes_file))\n",
    "\n",
    "        model.build_vocab(sentences)\n",
    "        print(\"training...\")\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        out_file = '/'.join(notes_file.split('/')[:-1] + [modelname])\n",
    "        print(\"writing embeddings to %s\" % (out_file))\n",
    "        model.save(out_file)\n",
    "        return out_file\n",
    "    \n",
    "    def gensim_to_fasttext_embeddings(self, wv_file, vocab_file, Y, outfile=None):\n",
    "        model = gensim.models.FastText.load(wv_file)\n",
    "        wv = model.wv\n",
    "        #free up memory\n",
    "        del model\n",
    "\n",
    "        vocab = set()\n",
    "        with open(vocab_file, 'r') as vocabfile:\n",
    "            for i,line in enumerate(vocabfile):\n",
    "                line = line.strip()\n",
    "                if line != '':\n",
    "                    vocab.add(line)\n",
    "        ind2w = {i+1:w for i,w in enumerate(sorted(vocab))}\n",
    "\n",
    "        W, words = self.build_matrix(ind2w, wv)\n",
    "\n",
    "        if outfile is None:\n",
    "            outfile = wv_file.replace('.fasttext', '.fasttext.embed')\n",
    "\n",
    "        #smash that save button\n",
    "        self.save_embeddings(W, words, outfile)\n",
    "    \n",
    "    def save_embeddings(self, W, words, outfile):\n",
    "        with open(outfile, 'w') as o:\n",
    "            #pad token already included\n",
    "            for i in range(len(words)):\n",
    "                line = [words[i]]\n",
    "                line.extend([str(d) for d in W[i]])\n",
    "                o.write(\" \".join(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything in one cell for easy running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_9ckqsQhqWyE"
   },
   "outputs": [],
   "source": [
    "#############Imports###############\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_ as xavier_uniform\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "from torchsummary import summary\n",
    "from entmax import sparsemax, entmax15, entmax_bisect\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertLayerNorm\n",
    "from pytorch_pretrained_bert import BertModel, BertConfig\n",
    "from pytorch_pretrained_bert import WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert import BertAdam\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "import transformers as tr\n",
    "from transformers import AdamW\n",
    "\n",
    "from allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer\n",
    "from allennlp.data import Token, Vocabulary, Instance\n",
    "from allennlp.data.fields import TextField\n",
    "from allennlp.data.dataset import Batch\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from typing import Tuple,Callable,IO,Optional\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "from functools import wraps\n",
    "from hashlib import sha256\n",
    "from typing import List\n",
    "from math import floor\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import tempfile\n",
    "import tarfile\n",
    "import random\n",
    "import shutil\n",
    "import struct\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "#Models\n",
    "\n",
    "\n",
    "class ModelHub:\n",
    "    \n",
    "    def __init__(self, args, dicts):\n",
    "        self.pick_model(args, dicts)\n",
    "        \n",
    "    def pick_model(self, args, dicts):\n",
    "        Y = len(dicts['ind2c'])\n",
    "        if args['model'] == 'KG_MultiResCNN':\n",
    "            model = KG_MultiResCNN(args, Y, dicts)\n",
    "        elif args['model'] == 'KG_MultiResCNNLSTM':\n",
    "            model = KG_MultiResCNNLSTM(args, Y, dicts)\n",
    "        elif args['model'] == 'bert_se_kg':\n",
    "            model = Bert_SE_KG(args, Y, dicts)\n",
    "        elif args['model'] == 'bert_we_kg':\n",
    "            model = Bert_WE_KG(args, Y, dicts)\n",
    "        elif args['model'] == 'bert_l4_we_kg':\n",
    "            model = Bert_L4_WE_KG(args, Y, dicts)\n",
    "        elif args['model'] == 'bert_mcnn_kg':\n",
    "            model = Bert_MCNN_KG(args, Y, dicts)\n",
    "        else:\n",
    "            raise RuntimeError(\"wrong model name\")\n",
    "\n",
    "        if args['test_model']:\n",
    "            sd = torch.load(args['test_model'])\n",
    "            model.load_state_dict(sd)\n",
    "        if args['gpu'] >= 0:\n",
    "            model.cuda(args['gpu'])\n",
    "        return model\n",
    "\n",
    "\n",
    "class WordRep(nn.Module):\n",
    "    def __init__(self, args, Y, dicts):\n",
    "        super(WordRep, self).__init__()\n",
    "\n",
    "        self.gpu = args['gpu']\n",
    "\n",
    "        self.isTfIdf = False\n",
    "        if args['use_tfIdf']:\n",
    "          self.isTfIdf = True\n",
    "\n",
    "        if args['embed_file']:\n",
    "            print(\"loading pretrained embeddings from {}\".format(args['embed_file']))\n",
    "            if args['use_ext_emb']:\n",
    "                pretrain_word_embedding, pretrain_emb_dim = self.build_pretrain_embedding(args['embed_file'], dicts['w2ind'],\n",
    "                                                                                     True)\n",
    "                W = torch.from_numpy(pretrain_word_embedding)\n",
    "            else:\n",
    "                W = torch.Tensor(self.load_embeddings(args['embed_file']))\n",
    "\n",
    "            self.embed = nn.Embedding(W.size()[0], W.size()[1], padding_idx=0)\n",
    "            self.embed.weight.data = W.clone()\n",
    "        else:\n",
    "            # add 2 to include UNK and PAD\n",
    "            self.embed = nn.Embedding(len(dicts['w2ind']) + 2, args['embed_size'], padding_idx=0)\n",
    "        self.feature_size = self.embed.embedding_dim\n",
    "\n",
    "        self.embed_drop = nn.Dropout(p=args['dropout'])\n",
    "\n",
    "        self.conv_dict = {\n",
    "                    1: [self.feature_size, args['num_filter_maps']],\n",
    "                    2: [self.feature_size, 100, args['num_filter_maps']],\n",
    "                    3: [self.feature_size, 150, 100, args['num_filter_maps']],\n",
    "                    4: [self.feature_size, 200, 150, 100, args['num_filter_maps']]\n",
    "                     }\n",
    "\n",
    "\n",
    "    def forward(self, x, tfIdf_inputs): #tfIdf_inputs\n",
    "\n",
    "        if self.gpu >= 0:\n",
    "            x = x if x.is_cuda else x.cuda(self.gpu)\n",
    "            if self.isTfIdf and tfIdf_inputs != None:\n",
    "                tfIdf_inputs = tfIdf_inputs if tfIdf_inputs.is_cuda else tfIdf_inputs.cuda(self.gpu)   \n",
    "        try:\n",
    "            features = [self.embed(x)]\n",
    "        except:\n",
    "            print(x)\n",
    "              raise\n",
    "\n",
    "        out = torch.cat(features, dim=2)\n",
    "\n",
    "        if self.isTfIdf and tfIdf_inputs != None:\n",
    "            weight = tfIdf_inputs.unsqueeze(dim=2)\n",
    "            out = out * weight\n",
    "\n",
    "        out = self.embed_drop(out)\n",
    "\n",
    "        del x\n",
    "        del tfIdf_inputs\n",
    "        return out\n",
    "    \n",
    "    def load_embeddings(self, embed_file):\n",
    "        #also normalizes the embeddings\n",
    "        W = []\n",
    "        with open(embed_file) as ef:\n",
    "            for line in ef:\n",
    "                line = line.rstrip().split()\n",
    "                vec = np.array(line[1:]).astype(np.float)\n",
    "                vec = vec / float(np.linalg.norm(vec) + 1e-6)\n",
    "                W.append(vec)\n",
    "            #UNK embedding, gaussian randomly initialized\n",
    "            print(\"adding unk embedding\")\n",
    "            vec = np.random.randn(len(W[-1]))\n",
    "            vec = vec / float(np.linalg.norm(vec) + 1e-6)\n",
    "            W.append(vec)\n",
    "        W = np.array(W)\n",
    "        return W\n",
    "    \n",
    "    def build_pretrain_embedding(self, embedding_path, word_alphabet, norm):\n",
    "        embedd_dict, embedd_dim = self.load_pretrain_emb(embedding_path)\n",
    "\n",
    "        scale = np.sqrt(3.0 / embedd_dim)\n",
    "        pretrain_emb = np.zeros([len(word_alphabet)+2, embedd_dim], dtype=np.float32)  # add UNK (last) and PAD (0)\n",
    "        perfect_match = 0\n",
    "        case_match = 0\n",
    "        digits_replaced_with_zeros_found = 0\n",
    "        lowercase_and_digits_replaced_with_zeros_found = 0\n",
    "        not_match = 0\n",
    "        for word, index in word_alphabet.items():\n",
    "            if word in embedd_dict:\n",
    "                if norm:\n",
    "                    pretrain_emb[index,:] = self.norm2one(embedd_dict[word])\n",
    "                else:\n",
    "                    pretrain_emb[index,:] = embedd_dict[word]\n",
    "                perfect_match += 1\n",
    "\n",
    "            elif word.lower() in embedd_dict:\n",
    "                if norm:\n",
    "                    pretrain_emb[index,:] = self.norm2one(embedd_dict[word.lower()])\n",
    "                else:\n",
    "                    pretrain_emb[index,:] = embedd_dict[word.lower()]\n",
    "                case_match += 1\n",
    "\n",
    "            elif re.sub('\\d', '0', word) in embedd_dict:\n",
    "                if norm:\n",
    "                    pretrain_emb[index,:] = self.norm2one(embedd_dict[re.sub('\\d', '0', word)])\n",
    "                else:\n",
    "                    pretrain_emb[index,:] = embedd_dict[re.sub('\\d', '0', word)]\n",
    "                digits_replaced_with_zeros_found += 1\n",
    "\n",
    "            elif re.sub('\\d', '0', word.lower()) in embedd_dict:\n",
    "                if norm:\n",
    "                    pretrain_emb[index,:] = self.norm2one(embedd_dict[re.sub('\\d', '0', word.lower())])\n",
    "                else:\n",
    "                    pretrain_emb[index,:] = embedd_dict[re.sub('\\d', '0', word.lower())]\n",
    "                lowercase_and_digits_replaced_with_zeros_found += 1\n",
    "\n",
    "            else:\n",
    "                if norm:\n",
    "                    pretrain_emb[index, :] = self.norm2one(np.random.uniform(-scale, scale, [1, embedd_dim]))\n",
    "                else:\n",
    "                    pretrain_emb[index,:] = np.random.uniform(-scale, scale, [1, embedd_dim])\n",
    "                not_match += 1\n",
    "\n",
    "        # initialize pad and unknown\n",
    "        pretrain_emb[0, :] = np.zeros([1, embedd_dim], dtype=np.float32)\n",
    "        if norm:\n",
    "            pretrain_emb[-1, :] = self.norm2one(np.random.uniform(-scale, scale, [1, embedd_dim]))\n",
    "        else:\n",
    "            pretrain_emb[-1, :] = np.random.uniform(-scale, scale, [1, embedd_dim])\n",
    "\n",
    "\n",
    "        print(\"pretrained word emb size {}\".format(len(embedd_dict)))\n",
    "        print(\"prefect match:%.2f%%, case_match:%.2f%%, dig_zero_match:%.2f%%, \"\n",
    "                     \"case_dig_zero_match:%.2f%%, not_match:%.2f%%\"\n",
    "                     %(perfect_match*100.0/len(word_alphabet), case_match*100.0/len(word_alphabet), digits_replaced_with_zeros_found*100.0/len(word_alphabet),\n",
    "                       lowercase_and_digits_replaced_with_zeros_found*100.0/len(word_alphabet), not_match*100.0/len(word_alphabet)))\n",
    "\n",
    "        return pretrain_emb, embedd_dim\n",
    "    \n",
    "    def load_pretrain_emb(self, embedding_path):\n",
    "        embedd_dim = -1\n",
    "        embedd_dict = dict()\n",
    "\n",
    "        # emb_debug = []\n",
    "        if embedding_path.find('.bin') != -1:\n",
    "            with open(embedding_path, 'rb') as f:\n",
    "                wordTotal = int(self._readString(f, 'utf-8'))\n",
    "                embedd_dim = int(self._readString(f, 'utf-8'))\n",
    "\n",
    "                for i in range(wordTotal):\n",
    "                    word = self._readString(f, 'utf-8')\n",
    "                    # emb_debug.append(word)\n",
    "\n",
    "                    word_vector = []\n",
    "                    for j in range(embedd_dim):\n",
    "                        word_vector.append(self._readFloat(f))\n",
    "                    word_vector = np.array(word_vector, np.float)\n",
    "\n",
    "                    f.read(1)  # a line break\n",
    "\n",
    "                    embedd_dict[word] = word_vector\n",
    "\n",
    "        else:\n",
    "            with codecs.open(embedding_path, 'r', 'UTF-8') as file:\n",
    "                for line in file:\n",
    "                    # logging.info(line)\n",
    "                    line = line.strip()\n",
    "                    if len(line) == 0:\n",
    "                        continue\n",
    "                    # tokens = line.split()\n",
    "                    tokens = re.split(r\"\\s+\", line)\n",
    "                    if len(tokens) == 2:\n",
    "                        continue # it's a head\n",
    "                    if embedd_dim < 0:\n",
    "                        embedd_dim = len(tokens) - 1\n",
    "                    else:\n",
    "                        # assert (embedd_dim + 1 == len(tokens))\n",
    "                        if embedd_dim + 1 != len(tokens):\n",
    "                            continue\n",
    "                    embedd = np.zeros([1, embedd_dim])\n",
    "                    embedd[:] = tokens[1:]\n",
    "                    embedd_dict[tokens[0]] = embedd\n",
    "\n",
    "\n",
    "        return embedd_dict, embedd_dim\n",
    "    \n",
    "    def _readString(self, f, code):\n",
    "        # s = unicode()\n",
    "        s = str()\n",
    "        c = f.read(1)\n",
    "        value = ord(c)\n",
    "\n",
    "        while value != 10 and value != 32:\n",
    "            if 0x00 < value < 0xbf:\n",
    "                continue_to_read = 0\n",
    "            elif 0xC0 < value < 0xDF:\n",
    "                continue_to_read = 1\n",
    "            elif 0xE0 < value < 0xEF:\n",
    "                continue_to_read = 2\n",
    "            elif 0xF0 < value < 0xF4:\n",
    "                continue_to_read = 3\n",
    "            else:\n",
    "                raise RuntimeError(\"not valid utf-8 code\")\n",
    "\n",
    "            i = 0\n",
    "            # temp = str()\n",
    "            # temp = temp + c\n",
    "\n",
    "            temp = bytes()\n",
    "            temp = temp + c\n",
    "\n",
    "            while i<continue_to_read:\n",
    "                temp = temp + f.read(1)\n",
    "                i += 1\n",
    "\n",
    "            temp = temp.decode(code)\n",
    "            s = s + temp\n",
    "\n",
    "            c = f.read(1)\n",
    "            value = ord(c)\n",
    "\n",
    "        return s\n",
    "    \n",
    "    def _readFloat(self,f):\n",
    "        bytes4 = f.read(4)\n",
    "        f_num = struct.unpack('f', bytes4)[0]\n",
    "        return f_num\n",
    "    \n",
    "    def norm2one(self,vec):\n",
    "        root_sum_square = np.sqrt(np.sum(np.square(vec)))\n",
    "        return vec/root_sum_square\n",
    "\n",
    "class SentimentOutput():\n",
    "    \n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.gpu = args['gpu']\n",
    "        \n",
    "        cache_path = os.path.join(args['bert_dir'], args['sentiment_bert'])\n",
    "        \n",
    "        savedModel = None\n",
    "        if os.path.exists(cache_path):\n",
    "            print(\"model path exist\")\n",
    "            savedModel = tr.AutoModelForSequenceClassification.from_pretrained(cache_path)\n",
    "        else:\n",
    "            print(\"Downloading and saving model\")\n",
    "            savedModel = tr.AutoModelForSequenceClassification.from_pretrained(str(args['sentiment_bert']))\n",
    "            savedModel.save_pretrained(save_directory = cache_path, save_config=True)\n",
    "        self.bert = savedModel\n",
    "        self.config = savedModel.config\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        encoded_input = dict()\n",
    "        if self.gpu >= 0:\n",
    "            x[0] = x[0] if x[0].is_cuda else x[0].cuda(self.gpu)\n",
    "            x[1] = x[1] if x[1].is_cuda else x[1].cuda(self.gpu)\n",
    "            model = self.bert\n",
    "            model = model.cuda(self.gpu)\n",
    "\n",
    "        encoded_input['input_ids'] = x[0]\n",
    "        encoded_input['attention_mask'] = x[1]\n",
    "        \n",
    "        senti_output = model(**encoded_input, output_hidden_states=True)\n",
    "        all_hidden_states  = senti_output.hidden_states\n",
    "        out = all_hidden_states[-1] #last hidden state. [#batch_size, sequence(m), 1024]\n",
    "\n",
    "        del all_hidden_states\n",
    "        del senti_output\n",
    "        del encoded_input\n",
    "        del x\n",
    "        del model\n",
    "\n",
    "        return out\n",
    "\n",
    "class OutputLayer(nn.Module):\n",
    "    def __init__(self, args, Y, dicts, input_size):\n",
    "        super(OutputLayer, self).__init__()\n",
    "\n",
    "        self.gpu = args['gpu']\n",
    "        \n",
    "        self.use_entmax15 = False\n",
    "        if args['use_entmax15']:\n",
    "            self.use_entmax15 = True\n",
    "\n",
    "        self.U = nn.Linear(input_size, Y)\n",
    "        xavier_uniform(self.U.weight)\n",
    "\n",
    "\n",
    "        self.final = nn.Linear(input_size, Y)\n",
    "        xavier_uniform(self.final.weight)\n",
    "\n",
    "        self.loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x, target):\n",
    "\n",
    "        if self.gpu >= 0:\n",
    "            target = target if target.is_cuda else target.cuda(self.gpu)\n",
    "            x = x if x.is_cuda else x.cuda(self.gpu)\n",
    "        \n",
    "        if self.use_entmax15:\n",
    "            alpha =  entmax15(self.U.weight.matmul(x.transpose(1, 2)), dim=2)\n",
    "        else:\n",
    "            alpha = F.softmax(self.U.weight.matmul(x.transpose(1, 2)), dim=2)\n",
    "\n",
    "        m = alpha.matmul(x)\n",
    "\n",
    "        y = self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
    "\n",
    "        loss = self.loss_function(y, target)\n",
    "\n",
    "        del x\n",
    "        del target\n",
    "\n",
    "        return y, loss\n",
    "\n",
    "class MRCNNLayer(nn.Module):\n",
    "    def __init__(self, args, feature_size):\n",
    "        super(MRCNNLayer, self).__init__()\n",
    "\n",
    "        self.gpu = args['gpu']\n",
    "\n",
    "        self.feature_size = feature_size\n",
    "        self.conv_dict = {\n",
    "            1: [self.feature_size, args['num_filter_maps']],\n",
    "            2: [self.feature_size, 100, args['num_filter_maps']],\n",
    "            3: [self.feature_size, 150, 100, args['num_filter_maps']],\n",
    "            4: [self.feature_size, 200, 150, 100, args['num_filter_maps']]\n",
    "              }\n",
    "        \n",
    "        self.conv = nn.ModuleList()\n",
    "        filter_sizes = args['filter_size'].split(',')\n",
    "\n",
    "        self.filter_num = len(filter_sizes)\n",
    "        for filter_size in filter_sizes:\n",
    "            filter_size = int(filter_size)\n",
    "            one_channel = nn.ModuleList()\n",
    "            \n",
    "            tmp = nn.Conv1d(self.feature_size, self.feature_size, kernel_size=filter_size,\n",
    "                            padding=int(floor(filter_size / 2)))\n",
    "            xavier_uniform(tmp.weight)\n",
    "            one_channel.add_module('baseconv', tmp)\n",
    "\n",
    "            conv_dimension = self.conv_dict[args['conv_layer']]\n",
    "            for idx in range(args['conv_layer']):\n",
    "                tmp = ResidualBlock(conv_dimension[idx], conv_dimension[idx + 1], filter_size, 1, True,\n",
    "                                    args['dropout'])\n",
    "                one_channel.add_module('resconv-{}'.format(idx), tmp)\n",
    "            \n",
    "            self.conv.add_module('channel-{}'.format(filter_size), one_channel)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.gpu >= 0:\n",
    "            x = x if x.is_cuda else x.cuda(self.gpu)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        conv_result = []\n",
    "        for conv in self.conv:\n",
    "            tmp = x\n",
    "            for idx, md in enumerate(conv):\n",
    "                if idx == 0:\n",
    "                    tmp = torch.tanh(md(tmp))\n",
    "                else:  \n",
    "                    tmp = md(tmp)\n",
    "            tmp = tmp.transpose(1, 2)\n",
    "            conv_result.append(tmp)\n",
    "        out = torch.cat(conv_result, dim=2)\n",
    "        del x\n",
    "        return out\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, kernel_size, stride, use_res, dropout):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv1d(inchannel, outchannel, kernel_size=kernel_size, stride=stride, padding=int(floor(kernel_size / 2)), bias=False),\n",
    "            nn.BatchNorm1d(outchannel),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(outchannel, outchannel, kernel_size=kernel_size, stride=1, padding=int(floor(kernel_size / 2)), bias=False),\n",
    "            nn.BatchNorm1d(outchannel)\n",
    "        )\n",
    "\n",
    "        self.use_res = use_res\n",
    "        if self.use_res:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                        nn.Conv1d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                        nn.BatchNorm1d(outchannel)\n",
    "                    )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        if self.use_res:\n",
    "            out += self.shortcut(x)   \n",
    "        out = torch.tanh(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class KG_MultiResCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, args, Y, dicts):\n",
    "        super(KG_MultiResCNN, self).__init__()\n",
    "\n",
    "        self.word_rep = WordRep(args, Y, dicts)\n",
    "        self.feature_size = self.word_rep.feature_size\n",
    "\n",
    "        self.is_sentiment = False\n",
    "        if args['use_sentiment']:\n",
    "            self.sentiment_model = SentimentOutput(args)\n",
    "            self.S_U = nn.Linear(self.sentiment_model.config.hidden_size, self.feature_size)\n",
    "            self.is_sentiment = True\n",
    "\n",
    "        if args['use_embd_layer'] and args['add_with_wordrap']:\n",
    "            self.kg_embd = EntityEmbedding(args, Y)\n",
    "            self.kg_embd.dim_red = nn.Linear(self.kg_embd.feature_size, self.feature_size)\n",
    "            self.kg_embd.feature_red = nn.Linear(args['MAX_ENT_LENGTH'], args['MAX_LENGTH'])\n",
    "            self.add_emb_with_wordrap = True\n",
    "\n",
    "        self.dropout = nn.Dropout(args['dropout'])\n",
    "\n",
    "        self.conv = MRCNNLayer(args, self.feature_size)\n",
    "\n",
    "        self.feature_size = self.conv.filter_num * args['num_filter_maps']\n",
    "\n",
    "        self.output_layer = OutputLayer(args, Y, dicts, self.feature_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, target, text_inputs, embeddings, tfIdf_inputs): #inputs_id, labels, text_inputs, embeddings, tfIdf_inputs\n",
    "\n",
    "        x = self.word_rep(x, tfIdf_inputs) #(batch, sequence, 100)\n",
    "\n",
    "        if self.is_sentiment:\n",
    "            senti_out = self.sentiment_model.forward(text_inputs)\n",
    "            s_alpha = self.S_U(senti_out)\n",
    "            del senti_out\n",
    "            x = torch.mul(x, s_alpha) #(batch, sequence, 100)\n",
    "            del s_alpha\n",
    "\n",
    "        if hasattr(self, 'add_emb_with_wordrap') and (self.add_emb_with_wordrap):\n",
    "            # with embedding layer\n",
    "            out = self.kg_embd(embeddings) #torch.Size([batch, seq len(n), embedding dim(200)])\n",
    "            out = self.kg_embd.dim_red(out) #torch.Size([batch, seq len(n), embedding dim(100)])\n",
    "\n",
    "            x = torch.cat((x, out), dim=1) # new shape (batch_size, sequence_length(m+n), feature_size (100))\n",
    "\n",
    "            del out\n",
    "\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv(x)\n",
    "\n",
    "\n",
    "        y, loss = self.output_layer(x, target)\n",
    "\n",
    "        del x\n",
    "\n",
    "        return y, loss\n",
    "\n",
    "    def freeze_net(self):\n",
    "        for p in self.word_rep.embed.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "class KG_MultiResCNNLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, args, Y, dicts):\n",
    "        super(KG_MultiResCNNLSTM, self).__init__()\n",
    "\n",
    "        self.word_rep = WordRep(args, Y, dicts)\n",
    "        self.embedding_size = self.word_rep.embed.weight.data.size()[0]\n",
    "        \n",
    "\n",
    "        self.conv = nn.ModuleList()\n",
    "        filter_sizes = args['filter_size'].split(',')\n",
    "\n",
    "        self.filter_num = len(filter_sizes)\n",
    "        for filter_size in filter_sizes:\n",
    "            filter_size = int(filter_size)\n",
    "            one_channel = nn.ModuleList()\n",
    "        \n",
    "            \n",
    "            tmp = nn.Conv1d(self.word_rep.feature_size, self.word_rep.feature_size, kernel_size=filter_size,\n",
    "                            padding=int(floor(filter_size / 2)))\n",
    "            xavier_uniform(tmp.weight)\n",
    "            one_channel.add_module('baseconv', tmp)\n",
    "\n",
    "            conv_dimension = self.word_rep.conv_dict[args['conv_layer']]\n",
    "            for idx in range(args['conv_layer']):\n",
    "                tmp = ResidualBlock(conv_dimension[idx], conv_dimension[idx + 1], filter_size, 1, True,\n",
    "                                    args['dropout'])\n",
    "                one_channel.add_module('resconv-{}'.format(idx), tmp)\n",
    "\n",
    "            lstm = torch.nn.LSTM(\n",
    "                    input_size= args['num_filter_maps'],\n",
    "                    hidden_size= args['num_filter_maps'],\n",
    "                    num_layers=1\n",
    "                )\n",
    "            \n",
    "            one_channel.add_module('LSTM', lstm)\n",
    "            \n",
    "            self.conv.add_module('channel-{}'.format(filter_size), one_channel)\n",
    "\n",
    "        self.output_layer = OutputLayer(args, Y, dicts, self.filter_num * args['num_filter_maps'])\n",
    "\n",
    "\n",
    "    def forward(self, x, target, text_inputs, embeddings, tfIdf_inputs):\n",
    "\n",
    "        x = self.word_rep(x, tfIdf_inputs)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        conv_result = []\n",
    "        for conv in self.conv:\n",
    "            tmp = x\n",
    "            for idx, md in enumerate(conv):\n",
    "                if idx == 0:\n",
    "                    tmp = torch.tanh(md(tmp))\n",
    "                else:\n",
    "                    if idx == 2:\n",
    "                        tmp = tmp.transpose(1, 2)\n",
    "                        tmp, (h,c) = md(tmp)\n",
    "                        tmp = tmp.transpose(1, 2)\n",
    "                    else:     \n",
    "                        tmp = md(tmp)\n",
    "            tmp = tmp.transpose(1, 2)\n",
    "            conv_result.append(tmp)\n",
    "        x = torch.cat(conv_result, dim=2)\n",
    "\n",
    "        y, loss = self.output_layer(x, target)\n",
    "\n",
    "        return y, loss\n",
    "\n",
    "    def freeze_net(self):\n",
    "        for p in self.word_rep.embed.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "\n",
    "class KGEntityToVec:\n",
    "    \n",
    "    @staticmethod\n",
    "    def getEntityToVec():\n",
    "        with open('%sentity2embedding.pickle' % args['out_path'], 'rb') as f:\n",
    "            entity2vec = pickle.load(f)\n",
    "        return entity2vec\n",
    "\n",
    "\n",
    "class EntityEmbedding(nn.Module):\n",
    "    def __init__(self, args, Y):\n",
    "        super(EntityEmbedding, self).__init__()\n",
    "\n",
    "        self.gpu = args['gpu']\n",
    "\n",
    "        entity2vec = KGEntityToVec().getEntityToVec()\n",
    "    \n",
    "        embedding_matrix = self.create_embedding_matrix(entity2vec)\n",
    "        \n",
    "        vocab_size=embedding_matrix.shape[0]\n",
    "        vector_size=embedding_matrix.shape[1]\n",
    "\n",
    "\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size,embedding_dim=vector_size)\n",
    "        self.embed.weight=nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n",
    "        # self.embed.weight.requires_grad=False\n",
    "        self.feature_size = self.embed.embedding_dim\n",
    "\n",
    "        self.conv_dict = {\n",
    "                    1: [self.feature_size, args['num_filter_maps']],\n",
    "                    2: [self.feature_size, 100, args['num_filter_maps']],\n",
    "                    3: [self.feature_size, 150, 100, args['num_filter_maps']],\n",
    "                    4: [self.feature_size, 200, 150, 100, args['num_filter_maps']]\n",
    "                     }\n",
    "\n",
    "        self.embed_drop = nn.Dropout(p=args['dropout'])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.gpu >= 0:\n",
    "            x = x if x.is_cuda else x.cuda(self.gpu)\n",
    "\n",
    "        features = [self.embed(x)]\n",
    "\n",
    "        output = torch.cat(features, dim=2)\n",
    "\n",
    "        output = self.embed_drop(output)\n",
    "\n",
    "        del x\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def create_embedding_matrix(self, ent2vec):\n",
    "        embedding_matrix=np.zeros((len(ent2vec)+2,200))\n",
    "\n",
    "        for index, key in enumerate(ent2vec):\n",
    "            vec = ent2vec[key]\n",
    "            vec = vec / float(np.linalg.norm(vec) + 1e-6)\n",
    "            embedding_matrix[index+1]=vec\n",
    "\n",
    "        return embedding_matrix\n",
    "\n",
    "class Bert_SE_KG(nn.Module): #bert with sentence embedding\n",
    "\n",
    "    def __init__(self, args, Y, dicts):\n",
    "        super(Bert_SE_KG, self).__init__()\n",
    "\n",
    "        cache_path = os.path.join(args['bert_dir'], args['pretrained_bert'])\n",
    "        \n",
    "        savedModel = None\n",
    "        if os.path.exists(cache_path):\n",
    "            print(\"model path exist\")\n",
    "            savedModel = tr.BertModel.from_pretrained(cache_path)\n",
    "        else:\n",
    "            print(\"Downloading and saving model\")\n",
    "            savedModel = tr.BertModel.from_pretrained(str(args['pretrained_bert']))\n",
    "            savedModel.save_pretrained(save_directory = cache_path, save_config=True)\n",
    "        self.bert = savedModel\n",
    "        self.config = savedModel.config\n",
    "        # print(\"Model config {}\".format(self.config))\n",
    "\n",
    "        self.feature_size = self.config.hidden_size\n",
    "\n",
    "        if args['use_embd_layer']:\n",
    "            self.kg_embd = EntityEmbedding(args, Y)\n",
    "            self.kg_embd.embed.weight.requires_grad=False\n",
    "            filetrs = [3]\n",
    "            self.convs = nn.ModuleList([nn.Conv1d(self.kg_embd.feature_size, self.kg_embd.feature_size, int(filter_size)) for filter_size in filetrs])\n",
    "            self.dim_reduction = nn.Linear(self.feature_size, self.kg_embd.feature_size)\n",
    "            self.feature_size = self.kg_embd.feature_size*2\n",
    "        \n",
    "        self.dropout = nn.Dropout(args['dropout'])\n",
    "        self.classifier = nn.Linear(self.feature_size, Y)\n",
    "        \n",
    "        \n",
    "        self.apply(self.init_bert_weights)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, entity_embeddings, target):\n",
    "\n",
    "        last_hidden_state, x = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, return_dict=False)\n",
    "        \n",
    "        if hasattr(self, 'kg_embd'):\n",
    "            # with embedding layer\n",
    "            out = self.kg_embd(entity_embeddings) #torch.Size([batch, seq len(n), embedding dim(200)])\n",
    "            \n",
    "            embedded = out.permute(0,2,1) #torch.Size([batch, embedding dim (200), seq len])#if want sentence embedding\n",
    "            conved = [torch.relu(conv(embedded)) for conv in self.convs] #if want sentence embedding\n",
    "            pooled = [conv.max(dim=-1).values for conv in conved] #if want sentence embedding\n",
    "            cat = self.dropout(torch.cat(pooled, dim=-1)) #if want sentence embedding\n",
    "\n",
    "            x = self.dim_reduction(x)\n",
    "            x = x / float(torch.norm(x) + 1e-6)\n",
    "\n",
    "            x = torch.cat((x, cat), dim=1) #if want sentence embedding\n",
    "        \n",
    "        x = self.dropout(x) #(batch_size, sequence_length(m), hidden_size(200/756))\n",
    "\n",
    "        y = self.classifier(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y, target)\n",
    "\n",
    "        return y, loss\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        BertLayerNorm = torch.nn.LayerNorm\n",
    "        \n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def freeze_net(self):\n",
    "        pass\n",
    "\n",
    "class Bert_WE_KG(nn.Module): #bert with word embedding\n",
    "\n",
    "    def __init__(self, args, Y, dicts):\n",
    "        super(Bert_WE_KG, self).__init__()\n",
    "\n",
    "        cache_path = os.path.join(args['bert_dir'], args['pretrained_bert'])\n",
    "        \n",
    "        savedModel = None\n",
    "        if os.path.exists(cache_path):\n",
    "            savedModel = tr.BertModel.from_pretrained(cache_path)\n",
    "        else:\n",
    "            savedModel = tr.BertModel.from_pretrained(str(args['pretrained_bert']))\n",
    "            savedModel.save_pretrained(save_directory = cache_path, save_config=True)\n",
    "        self.bert = savedModel\n",
    "        self.config = savedModel.config\n",
    "\n",
    "        self.feature_size = self.config.hidden_size\n",
    "\n",
    "        if args['use_embd_layer']:\n",
    "            self.kg_embd = EntityEmbedding(args, Y)\n",
    "            self.kg_embd.embed.weight.requires_grad=False\n",
    "            self.dim_reduction = nn.Linear(self.feature_size, self.kg_embd.feature_size)\n",
    "            self.feature_size = self.kg_embd.feature_size\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(args['dropout'])\n",
    "\n",
    "        self.output_layer = OutputLayer(args, Y, dicts, self.feature_size)\n",
    "        \n",
    "        \n",
    "        self.apply(self.init_bert_weights)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, entity_embeddings, target):\n",
    "\n",
    "        last_hidden_state, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, return_dict=False)\n",
    "\n",
    "        x = self.dropout(last_hidden_state) #(batch_size, sequence_length(m), hidden_size(786))\n",
    "\n",
    "        \n",
    "        if hasattr(self, 'kg_embd'):\n",
    "            out = self.kg_embd(entity_embeddings) #torch.Size([batch, seq len(n), embedding dim(200)])\n",
    "\n",
    "            x = self.dim_reduction(x) #torch.Size([batch, seq len(m), embedding dim(200)])\n",
    "\n",
    "            x = x / float(torch.norm(x) + 1e-6)\n",
    "            x = torch.cat((x, out), dim=1) # new shape (batch_size, sequence_length(m+n), feature_size (200))\n",
    "\n",
    "\n",
    "        y, loss = self.output_layer(x, target)\n",
    "        \n",
    "        \n",
    "        return y, loss\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        BertLayerNorm = torch.nn.LayerNorm\n",
    "        \n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def freeze_net(self):\n",
    "        pass\n",
    "\n",
    "class Bert_L4_WE_KG(nn.Module): #adding last 5 layers output of bert\n",
    "\n",
    "    def __init__(self, args, Y, dicts):\n",
    "        super(Bert_L4_WE_KG, self).__init__()\n",
    "\n",
    "        cache_path = os.path.join(args['bert_dir'], args['pretrained_bert'])\n",
    "        \n",
    "        savedModel = None\n",
    "        if os.path.exists(cache_path):\n",
    "            savedModel = tr.BertModel.from_pretrained(cache_path, return_dict=True)\n",
    "        else:\n",
    "            savedModel = tr.BertModel.from_pretrained(str(args['pretrained_bert']), return_dict=True)\n",
    "            savedModel.save_pretrained(save_directory = cache_path, save_config=True)\n",
    "        self.bert = savedModel\n",
    "        self.config = savedModel.config\n",
    "\n",
    "\n",
    "        self.feature_size = self.config.hidden_size*4\n",
    "\n",
    "        if args['use_embd_layer']:\n",
    "            self.kg_embd = EntityEmbedding(args, Y)\n",
    "            self.kg_embd.embed.weight.requires_grad=False\n",
    "            self.dim_reduction = nn.Linear(self.feature_size, self.kg_embd.feature_size)\n",
    "            self.feature_size = self.kg_embd.feature_size\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(args['dropout'])\n",
    "        self.output_layer = OutputLayer(args, Y, dicts, self.feature_size)\n",
    "        self.apply(self.init_bert_weights)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, entity_embeddings, target):\n",
    "\n",
    "        output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        \n",
    "\n",
    "        #*************experiment*************\n",
    "        hidden_states = output.hidden_states\n",
    "        # concatenate last four layers\n",
    "        x = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1) #[batch_size, sequence_length, hidden_size(786)*4]\n",
    "        #***********experiment***************\n",
    "\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        if hasattr(self, 'kg_embd'):\n",
    "            out = self.kg_embd(entity_embeddings) #torch.Size([batch, seq len(n), embedding dim(200)])\n",
    "\n",
    "            x = self.dim_reduction(x) #torch.Size([batch, seq len(m), embedding dim(200)])\n",
    "\n",
    "            x = x / float(torch.norm(x) + 1e-6)\n",
    "            x = torch.cat((x, out), dim=1) # new shape (batch_size, sequence_length(m+n), feature_size (200))\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        y, loss = self.output_layer(x, target)\n",
    "        \n",
    "        return y, loss\n",
    "\n",
    "    def loss_fn(self, outputs, target):\n",
    "        return nn.BCEWithLogitsLoss()(outputs, target)\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        BertLayerNorm = torch.nn.LayerNorm\n",
    "        \n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def freeze_net(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Bert_MCNN_KG(nn.Module): #Bert with KG and CNN\n",
    "\n",
    "    def __init__(self, args, Y, dicts):\n",
    "        super(Bert_MCNN_KG, self).__init__()\n",
    "\n",
    "        cache_path = os.path.join(args['bert_dir'], args['pretrained_bert'])\n",
    "        \n",
    "        savedModel = None\n",
    "        if os.path.exists(cache_path):\n",
    "            savedModel = tr.BertModel.from_pretrained(cache_path)\n",
    "        else:\n",
    "            savedModel = tr.BertModel.from_pretrained(str(args['pretrained_bert']))\n",
    "            savedModel.save_pretrained(save_directory = cache_path, save_config=True)\n",
    "        self.bert = savedModel\n",
    "        self.config = savedModel.config\n",
    "\n",
    "        self.dim_reduction1 = nn.Linear(self.config.hidden_size*4, self.config.hidden_size)\n",
    "\n",
    "        self.feature_size = self.config.hidden_size\n",
    "\n",
    "        if args['use_embd_layer']:\n",
    "            self.kg_embd = EntityEmbedding(args, Y)\n",
    "            self.kg_embd.embed.weight.requires_grad=False\n",
    "            self.dim_reduction2 = nn.Linear(self.feature_size, self.kg_embd.feature_size)\n",
    "            self.feature_size = self.kg_embd.feature_size\n",
    "        \n",
    "        self.dropout = nn.Dropout(args['dropout'])\n",
    "\n",
    "        self.conv = MRCNNLayer(args, self.feature_size)\n",
    "\n",
    "\n",
    "        self.feature_size = self.conv.filter_num * args['num_filter_maps']\n",
    "\n",
    "        self.output_layer = OutputLayer(args, Y, dicts, self.feature_size)\n",
    "      \n",
    "        \n",
    "        # self.apply(self.init_bert_weights)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, entity_embeddings, target):\n",
    "\n",
    "        output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "\n",
    "        #*************experiment*************\n",
    "        hidden_states = output.hidden_states\n",
    "        # concatenate last four layers\n",
    "        x = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1) #[batch_size, sequence_length, hidden_size(786)*4]\n",
    "        #***********experiment***************\n",
    "\n",
    "        x = x / float(torch.norm(x) + 1e-6) #normalize\n",
    "        x = self.dim_reduction1(x) #[batch_size, sequence_length, hidden_size(786]\n",
    "\n",
    "        if hasattr(self, 'kg_embd'):\n",
    "            out = self.kg_embd(entity_embeddings) #torch.Size([batch, seq len(n), embedding dim(200)])\n",
    "            x = self.dim_reduction2(x)\n",
    "            x = torch.cat((x, out), dim=1) # new shape (batch_size, sequence_length(m+n), feature_size (200))\n",
    "\n",
    "        x = self.dropout(x) #(batch_size, sequence_length, hidden_size(786 or 200))\n",
    "\n",
    "        x = self.conv(x)\n",
    "\n",
    "        y, loss = self.output_layer(x, target)\n",
    "        \n",
    "        return y, loss\n",
    "\n",
    "    def freeze_net(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#############Train-Test###############\n",
    "\n",
    "class Train_Test:\n",
    "    def __init__(self):\n",
    "        print(\"Train--Test\")\n",
    "    \n",
    "    def train(self, args, model, optimizer, scheduler, epoch, gpu, data_loader):\n",
    "        # print(\"EPOCH %d\" % epoch)\n",
    "        print('Epoch:', epoch,'LR:', optimizer.param_groups[0]['lr'])\n",
    "        print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
    "\n",
    "        losses = []\n",
    "\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # loader\n",
    "        data_iter = iter(data_loader)\n",
    "        num_iter = len(data_loader)\n",
    "        \n",
    "        for i in tqdm(range(num_iter)):\n",
    "\n",
    "            if args['model'].find(\"bert\") != -1:\n",
    "\n",
    "                inputs_id, segments, masks, ent_embeddings, labels = next(data_iter)\n",
    "\n",
    "                inputs_id, segments, masks, labels = torch.LongTensor(inputs_id), torch.LongTensor(segments), \\\n",
    "                                                     torch.LongTensor(masks), torch.FloatTensor(labels)\n",
    "                if args['use_embd_layer']:\n",
    "                    #for embedding layer\n",
    "                    ent_embeddings = torch.LongTensor(ent_embeddings)\n",
    "                else:\n",
    "                    ent_embeddings = None\n",
    "\n",
    "                if gpu >= 0:\n",
    "                    if args['use_embd_layer']:\n",
    "                        ent_embeddings = ent_embeddings.cuda(gpu)\n",
    "                    else:\n",
    "                        ent_embeddings = None\n",
    "                        \n",
    "                    inputs_id, segments, masks, labels = inputs_id.cuda(gpu), segments.cuda(gpu), \\\n",
    "                                                         masks.cuda(gpu), labels.cuda(gpu)\n",
    "                try:\n",
    "                    optimizer.zero_grad()\n",
    "                    output, loss = model(inputs_id, segments, masks, ent_embeddings, labels)\n",
    "                except:\n",
    "                    print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "                    raise\n",
    "                \n",
    "            else:\n",
    "\n",
    "                inputs_id, labels, text_inputs, embeddings, tfIdf_inputs = next(data_iter)\n",
    "\n",
    "                if args['use_embd_layer']:\n",
    "                    embeddings = torch.LongTensor(embeddings)\n",
    "\n",
    "                if args['use_sentiment']:\n",
    "                    input_ids = torch.stack([x_[0][0] for x_ in text_inputs])\n",
    "                    attention = torch.stack([x_[1][0] for x_ in text_inputs])\n",
    "                    text_inputs = [input_ids,attention]\n",
    "\n",
    "                if args['use_tfIdf']:\n",
    "                    tfIdf_inputs = torch.FloatTensor(tfIdf_inputs)\n",
    "\n",
    "                inputs_id, labels = torch.LongTensor(inputs_id), torch.FloatTensor(labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output, loss = model(inputs_id, labels, text_inputs, embeddings, tfIdf_inputs)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if args['grad_clip']:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "          \n",
    "        return losses\n",
    "    \n",
    "    def test(self, args, model, data_path, fold, gpu, dicts, data_loader):\n",
    "        self.model_name = args['model']\n",
    "        filename = data_path.replace('train', fold)\n",
    "        print('file for evaluation: %s' % filename)\n",
    "        num_labels = len(dicts['ind2c'])\n",
    "\n",
    "        y, yhat, yhat_raw, hids, losses = [], [], [], [], []\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # loader\n",
    "        data_iter = iter(data_loader)\n",
    "        num_iter = len(data_loader)\n",
    "        for i in tqdm(range(num_iter)):\n",
    "            with torch.no_grad():\n",
    "\n",
    "                if args['model'].find(\"bert\") != -1:\n",
    "                    inputs_id, segments, masks, ent_embeddings, labels = next(data_iter)\n",
    "\n",
    "                    inputs_id, segments, masks, labels = torch.LongTensor(inputs_id), torch.LongTensor(segments), \\\n",
    "                                                         torch.LongTensor(masks), torch.FloatTensor(labels)\n",
    "                    \n",
    "                    \n",
    "                    if args['use_embd_layer']:\n",
    "                        #for embedding layer\n",
    "                        ent_embeddings = torch.LongTensor(ent_embeddings)\n",
    "                    else:\n",
    "                        ent_embeddings = None\n",
    "\n",
    "                    if gpu >= 0:\n",
    "                        if args['use_embd_layer']:\n",
    "                            ent_embeddings = ent_embeddings.cuda(gpu)\n",
    "                        else:\n",
    "                            ent_embeddings = None\n",
    "                        inputs_id, segments, masks, labels = inputs_id.cuda(\n",
    "                            gpu), segments.cuda(gpu), masks.cuda(gpu), labels.cuda(gpu)\n",
    "\n",
    "                    try:\n",
    "                        output, loss = model(inputs_id, segments, masks, ent_embeddings, labels)\n",
    "                    except:\n",
    "                        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "                        raise\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    inputs_id, labels, text_inputs, embeddings, tfIdf_inputs = next(data_iter)\n",
    "\n",
    "                    if args['use_embd_layer']:\n",
    "                        embeddings = torch.LongTensor(embeddings)\n",
    "\n",
    "                    if args['use_sentiment']:\n",
    "                        input_ids = torch.stack([x_[0][0] for x_ in text_inputs])\n",
    "                        attention = torch.stack([x_[1][0] for x_ in text_inputs])\n",
    "                        text_inputs = [input_ids,attention]\n",
    "\n",
    "                    if args['use_tfIdf']:\n",
    "                        tfIdf_inputs = torch.FloatTensor(tfIdf_inputs)\n",
    "\n",
    "                    inputs_id, labels = torch.LongTensor(inputs_id), torch.FloatTensor(labels)\n",
    "\n",
    "                    output, loss = model(inputs_id, labels, text_inputs, embeddings, tfIdf_inputs)\n",
    "\n",
    "                output = torch.sigmoid(output)\n",
    "                output = output.data.cpu().numpy()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                target_data = labels.data.cpu().numpy()\n",
    "\n",
    "                yhat_raw.append(output)\n",
    "                \n",
    "                output = np.round(output)\n",
    "                \n",
    "                y.append(target_data)\n",
    "                \n",
    "                yhat.append(output)\n",
    "\n",
    "        y = np.concatenate(y, axis=0)\n",
    "        yhat = np.concatenate(yhat, axis=0)\n",
    "        yhat_raw = np.concatenate(yhat_raw, axis=0)\n",
    "\n",
    "\n",
    "        k = 5 if num_labels == 50 else [8,15]\n",
    "\n",
    "        self.new_metric_calc(y, yhat_raw) #checking my metric values #considering 0 detection as TN\n",
    "\n",
    "        self.calculate_print_metrics(y, yhat_raw) #checking sklearn metric values considering 0 detection as TP\n",
    "        \n",
    "        metrics = self.all_metrics(yhat, y, k=k, yhat_raw=yhat_raw)\n",
    "\n",
    "        print()\n",
    "        print(\"Metric calculation by Fei Li and Hong Yu start\")\n",
    "        self.print_metrics(metrics)\n",
    "        print(\"Metric calculation by Fei Li and Hong Yu end\")\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        metrics['loss_%s' % fold] = np.mean(losses)\n",
    "\n",
    "        print('loss_%s' % fold, metrics['loss_%s' % fold])\n",
    "        return metrics\n",
    "    \n",
    "    def new_metric_calc(self, y, yhat):\n",
    "        names = [\"acc\", \"prec\", \"rec\", \"f1\"]\n",
    "        \n",
    "        yhat = np.round(yhat) #rounding the vaues\n",
    "        \n",
    "        #Macro\n",
    "        macro_accuracy = np.mean([accuracy_score(y[i], yhat[i]) for i in range(len(y))])\n",
    "        macro_precision = np.mean([self.getPrecision(y[i], yhat[i]) for i in range(len(y))])\n",
    "        macro_recall = np.mean([self.getRecall(y[i], yhat[i]) for i in range(len(y))])\n",
    "        macro_f_score = np.mean([self.getFScore(y[i], yhat[i]) for i in range(len(y))])\n",
    "        \n",
    "        \n",
    "        #Micro\n",
    "        ymic = y.ravel()\n",
    "        yhatmic = yhat.ravel()\n",
    "        \n",
    "        micro_accuracy =  accuracy_score(ymic, yhatmic)\n",
    "        micro_precision =  self.getPrecision(ymic, yhatmic)\n",
    "        micro_recall =  self.getRecall(ymic, yhatmic)\n",
    "        micro_f_score =  self.getFScore(ymic, yhatmic)\n",
    "        \n",
    "        \n",
    "        macro = (macro_accuracy, macro_precision, macro_recall, macro_f_score)\n",
    "        micro = (micro_accuracy, micro_precision, micro_recall, micro_f_score)\n",
    "        \n",
    "        metrics = {names[i] + \"_macro\": macro[i] for i in range(len(macro))}\n",
    "        metrics.update({names[i] + \"_micro\": micro[i] for i in range(len(micro))})\n",
    "\n",
    "        print()\n",
    "        print(\"Metric calculation for all labels together start\")\n",
    "        self.print_metrics(metrics)\n",
    "        print(\"Metric calculation for all labels together end\")\n",
    "        print()\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def getFScore(self, y, yhat):\n",
    "        prec = self.getPrecision(y, yhat)\n",
    "        rec = self.getRecall(y, yhat)\n",
    "        if prec + rec == 0:\n",
    "            f1 = 0.\n",
    "        else:\n",
    "            f1 = (2*(prec*rec))/(prec+rec)\n",
    "        \n",
    "        return f1\n",
    "    \n",
    "    def getRecall(self, y, yhat):\n",
    "        return self.getTP(y, yhat)/(self.getTP(y, yhat) + self.getFN(y, yhat) + 1e-10)\n",
    "    \n",
    "    def getPrecision(self, y, yhat):\n",
    "        return self.getTP(y, yhat)/(self.getTP(y, yhat) + self.getFP(y, yhat) + 1e-10)\n",
    "    \n",
    "    def getTP(self, y, yhat):\n",
    "        return np.multiply(y, yhat).sum().item()\n",
    "    \n",
    "    def getFN(self, y, yhat):\n",
    "        return np.multiply(y, np.logical_not(yhat).astype(float)).sum().item()\n",
    "    \n",
    "    def getFP(self, y, yhat):\n",
    "        return np.multiply(np.logical_not(y).astype(float), y).sum().item()\n",
    "    \n",
    "    def calculate_print_metrics(self, y, yhat):\n",
    "        \n",
    "        names = [\"acc\", \"prec\", \"rec\", \"f1\"]\n",
    "        \n",
    "        yhat = np.round(yhat) #rounding the vaues\n",
    "        \n",
    "        macro_precision, macro_recall, macro_f_score, macro_support = precision_recall_fscore_support(y, yhat, average = 'macro', zero_division=1)\n",
    "#         macro_accuracy = ((np.concatenate(np.round(yhat), axis=0) == np.concatenate(y, axis=0)).sum().item()) / len(y) #accuracy_score(y, np.round(yhat))\n",
    "#         macro_accuracy = ((np.round(yhat) == y).sum().item() / len(y[0])) / len(y)\n",
    "        macro_accuracy = np.mean([accuracy_score(y[i], yhat[i]) for i in range(len(y))])\n",
    "        \n",
    "        \n",
    "        ymic = y.ravel()\n",
    "        yhatmic = yhat.ravel()\n",
    "        micro_precision, micro_recall, micro_f_score, micro_support = precision_recall_fscore_support(ymic, yhatmic, average='micro', zero_division=1)\n",
    "        micro_accuracy =  accuracy_score(ymic, yhatmic) \n",
    "        \n",
    "        macro = (macro_accuracy, macro_precision, macro_recall, macro_f_score)\n",
    "        micro = (micro_accuracy, micro_precision, micro_recall, micro_f_score)\n",
    "        \n",
    "        metrics = {names[i] + \"_macro\": macro[i] for i in range(len(macro))}\n",
    "        metrics.update({names[i] + \"_micro\": micro[i] for i in range(len(micro))})\n",
    "        \n",
    "        \n",
    "        print()\n",
    "        print(\"Sklearn Metric calculation start\")\n",
    "        self.print_metrics(metrics)\n",
    "        print(\"Sklearn Metric calculation end\")\n",
    "        print()\n",
    "\n",
    "        return metrics\n",
    "        \n",
    "    \n",
    "    def all_metrics(self, yhat, y, k=8, yhat_raw=None, calc_auc=True):\n",
    "        \"\"\"\n",
    "            Inputs:\n",
    "                yhat: binary predictions matrix\n",
    "                y: binary ground truth matrix\n",
    "                k: for @k metrics\n",
    "                yhat_raw: prediction scores matrix (floats)\n",
    "            Outputs:\n",
    "                dict holding relevant metrics\n",
    "        \"\"\"\n",
    "        names = [\"acc\", \"prec\", \"rec\", \"f1\"]\n",
    "\n",
    "        #macro\n",
    "        macro = self.all_macro(yhat, y)\n",
    "        #micro\n",
    "        ymic = y.ravel()\n",
    "        yhatmic = yhat.ravel()\n",
    "        micro = self.all_micro(yhatmic, ymic)\n",
    "\n",
    "        metrics = {names[i] + \"_macro\": macro[i] for i in range(len(macro))}\n",
    "        metrics.update({names[i] + \"_micro\": micro[i] for i in range(len(micro))})\n",
    "\n",
    "        #AUC and @k\n",
    "        if yhat_raw is not None and calc_auc:\n",
    "            #allow k to be passed as int or list\n",
    "            if type(k) != list:\n",
    "                k = [k]\n",
    "            for k_i in k:\n",
    "                rec_at_k = self.recall_at_k(yhat_raw, y, k_i)\n",
    "                metrics['rec_at_%d' % k_i] = rec_at_k\n",
    "                prec_at_k = self.precision_at_k(yhat_raw, y, k_i)\n",
    "                metrics['prec_at_%d' % k_i] = prec_at_k\n",
    "                metrics['f1_at_%d' % k_i] = 2*(prec_at_k*rec_at_k)/(prec_at_k+rec_at_k)\n",
    "\n",
    "            roc_auc = self.auc_metrics(yhat_raw, y, ymic)\n",
    "            metrics.update(roc_auc)\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def auc_metrics(self, yhat_raw, y, ymic):\n",
    "        if yhat_raw.shape[0] <= 1:\n",
    "            return\n",
    "        fpr = {}\n",
    "        tpr = {}\n",
    "        roc_auc = {}\n",
    "        #get AUC for each label individually\n",
    "        relevant_labels = []\n",
    "        auc_labels = {}\n",
    "        for i in range(y.shape[1]):\n",
    "            #only if there are true positives for this label\n",
    "            if y[:,i].sum() > 0:\n",
    "                fpr[i], tpr[i], _ = roc_curve(y[:,i], yhat_raw[:,i])\n",
    "                if len(fpr[i]) > 1 and len(tpr[i]) > 1:\n",
    "                    auc_score = auc(fpr[i], tpr[i])\n",
    "                    if not np.isnan(auc_score):\n",
    "                        auc_labels[\"auc_%d\" % i] = auc_score\n",
    "                        relevant_labels.append(i)\n",
    "\n",
    "        #macro-AUC: just average the auc scores\n",
    "        aucs = []\n",
    "        for i in relevant_labels:\n",
    "            aucs.append(auc_labels['auc_%d' % i])\n",
    "        roc_auc['auc_macro'] = np.mean(aucs)\n",
    "\n",
    "        #micro-AUC: just look at each individual prediction\n",
    "        yhatmic = yhat_raw.ravel()\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ymic, yhatmic)\n",
    "        roc_auc[\"auc_micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        return roc_auc\n",
    "    \n",
    "    def precision_at_k(self, yhat_raw, y, k):\n",
    "        #num true labels in top k predictions / k\n",
    "        sortd = np.argsort(yhat_raw)[:,::-1]\n",
    "        topk = sortd[:,:k]\n",
    "\n",
    "        #get precision at k for each example\n",
    "        vals = []\n",
    "        for i, tk in enumerate(topk):\n",
    "            if len(tk) > 0:\n",
    "                num_true_in_top_k = y[i,tk].sum()\n",
    "                denom = len(tk)\n",
    "                vals.append(num_true_in_top_k / float(denom))\n",
    "\n",
    "        return np.mean(vals)\n",
    "    \n",
    "    def recall_at_k(self,yhat_raw, y, k):\n",
    "        #num true labels in top k predictions / num true labels\n",
    "        sortd = np.argsort(yhat_raw)[:,::-1]\n",
    "        topk = sortd[:,:k]\n",
    "\n",
    "        #get recall at k for each example\n",
    "        vals = []\n",
    "        for i, tk in enumerate(topk):\n",
    "            num_true_in_top_k = y[i,tk].sum()\n",
    "            denom = y[i,:].sum()\n",
    "            vals.append(num_true_in_top_k / float(denom))\n",
    "\n",
    "        vals = np.array(vals)\n",
    "        vals[np.isnan(vals)] = 0.\n",
    "\n",
    "        return np.mean(vals)\n",
    "    \n",
    "    def all_micro(self, yhatmic, ymic):\n",
    "        return self.micro_accuracy(yhatmic, ymic), self.micro_precision(yhatmic, ymic), self.micro_recall(yhatmic, ymic), self.micro_f1(yhatmic, ymic)\n",
    "    \n",
    "    def micro_f1(self, yhatmic, ymic):\n",
    "        prec = self.micro_precision(yhatmic, ymic)\n",
    "        rec = self.micro_recall(yhatmic, ymic)\n",
    "        if prec + rec == 0:\n",
    "            f1 = 0.\n",
    "        else:\n",
    "            f1 = 2*(prec*rec)/(prec+rec)\n",
    "        return f1\n",
    "    \n",
    "    def micro_recall(self, yhatmic, ymic):\n",
    "        return self.intersect_size(yhatmic, ymic, 0) / (ymic.sum(axis=0) + 1e-10) #NaN fix\n",
    "    \n",
    "    def micro_precision(self, yhatmic, ymic):\n",
    "        return self.intersect_size(yhatmic, ymic, 0) / (yhatmic.sum(axis=0) + 1e-10) #NaN fix\n",
    "    \n",
    "    def micro_accuracy(self, yhatmic, ymic):\n",
    "        return self.intersect_size(yhatmic, ymic, 0) / (self.union_size(yhatmic, ymic, 0) + 1e-10) #NaN fix\n",
    "    \n",
    "    def all_macro(self,yhat, y):\n",
    "        return self.macro_accuracy(yhat, y), self.macro_precision(yhat, y), self.macro_recall(yhat, y), self.macro_f1(yhat, y)\n",
    "    \n",
    "    def macro_f1(self, yhat, y):\n",
    "        prec = self.macro_precision(yhat, y)\n",
    "        rec = self.macro_recall(yhat, y)\n",
    "        if prec + rec == 0:\n",
    "            f1 = 0.\n",
    "        else:\n",
    "            f1 = 2*(prec*rec)/(prec+rec)\n",
    "        return f1\n",
    "    \n",
    "    def macro_recall(self, yhat, y):\n",
    "        num = self.intersect_size(yhat, y, 0) / (y.sum(axis=0) + 1e-10)\n",
    "        return np.mean(num)\n",
    "    \n",
    "    def macro_precision(self, yhat, y):\n",
    "        num = self.intersect_size(yhat, y, 0) / (yhat.sum(axis=0) + 1e-10)\n",
    "        return np.mean(num)\n",
    "    \n",
    "    def macro_accuracy(self, yhat, y):\n",
    "        num = self.intersect_size(yhat, y, 0) / (self.union_size(yhat, y, 0) + 1e-10)\n",
    "        return np.mean(num)\n",
    "    \n",
    "    def intersect_size(self, yhat, y, axis):\n",
    "        #axis=0 for label-level union (macro). axis=1 for instance-level\n",
    "        return np.logical_and(yhat, y).sum(axis=axis).astype(float)\n",
    "    \n",
    "    def union_size(self, yhat, y, axis):\n",
    "        #axis=0 for label-level union (macro). axis=1 for instance-level\n",
    "        return np.logical_or(yhat, y).sum(axis=axis).astype(float)\n",
    "    \n",
    "    def print_metrics(self, metrics):\n",
    "        print()\n",
    "        if \"auc_macro\" in metrics.keys():\n",
    "            print(\"[MACRO] accuracy, precision, recall, f-measure, AUC\")\n",
    "            print(\"   %.4f, %.4f, %.4f, %.4f, %.4f\" % (metrics[\"acc_macro\"], metrics[\"prec_macro\"], metrics[\"rec_macro\"], metrics[\"f1_macro\"], metrics[\"auc_macro\"]))\n",
    "        else:\n",
    "            print(\"[MACRO] accuracy, precision, recall, f-measure\")\n",
    "            print(\"   %.4f, %.4f, %.4f, %.4f\" % (metrics[\"acc_macro\"], metrics[\"prec_macro\"], metrics[\"rec_macro\"], metrics[\"f1_macro\"]))\n",
    "\n",
    "        if \"auc_micro\" in metrics.keys():\n",
    "            print(\"[MICRO] accuracy, precision, recall, f-measure, AUC\")\n",
    "            print(\"   %.4f, %.4f, %.4f, %.4f, %.4f\" % (metrics[\"acc_micro\"], metrics[\"prec_micro\"], metrics[\"rec_micro\"], metrics[\"f1_micro\"], metrics[\"auc_micro\"]))\n",
    "        else:\n",
    "            print(\"[MICRO] accuracy, precision, recall, f-measure\")\n",
    "            print(\"   %.4f, %.4f, %.4f, %.4f\" % (metrics[\"acc_micro\"], metrics[\"prec_micro\"], metrics[\"rec_micro\"], metrics[\"f1_micro\"]))\n",
    "        for metric, val in metrics.items():\n",
    "            if metric.find(\"rec_at\") != -1:\n",
    "                print(\"%s: %.4f\" % (metric, val))\n",
    "        print()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "#############Model Summary###############\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def My_Summary(model, input_size, batch_size=-1, device=\"cuda\"):\n",
    "\n",
    "    def register_hook(module):\n",
    "\n",
    "        def hook(module, input, output):\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "\n",
    "            m_key = \"%s-%i\" % (class_name, module_idx + 1)\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"input_shape\"][0] = batch_size\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [[-1] + list(o.size())[1:] for o in output if len(list(o.size())) > 0][0]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                if len(summary[m_key][\"output_shape\"]) > 0:\n",
    "                    summary[m_key][\"output_shape\"][0] = batch_size\n",
    "                else:\n",
    "                    summary[m_key][\"output_shape\"] = [-1]\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "            not isinstance(module, nn.Sequential)\n",
    "            and not isinstance(module, nn.ModuleList)\n",
    "            and not (module == model)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    device = device.lower()\n",
    "    assert device in [\n",
    "        \"cuda\",\n",
    "        \"cpu\",\n",
    "    ], \"Input device is not valid, please specify 'cuda' or 'cpu'\"\n",
    "\n",
    "    # if device == \"cuda\" and torch.cuda.is_available():\n",
    "    #     dtype = torch.cuda.FloatTensor\n",
    "    # else:\n",
    "    #     dtype = torch.FloatTensor\n",
    "\n",
    "    # multiple inputs to the network\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "\n",
    "    # batch_size of 2 for batchnorm\n",
    "    x = [torch.rand(2, *in_size[0]).type(in_size[1]) if in_size[1] != 0 else None for in_size in input_size]\n",
    "    # print(type(x[0]))\n",
    "\n",
    "    # create properties\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    # register hook\n",
    "    model.apply(register_hook)\n",
    "\n",
    "    # make a forward pass\n",
    "    # print(x.shape)\n",
    "    model(*x)\n",
    "\n",
    "    # remove these hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer (type)\", \"Output Shape\", \"Param #\")\n",
    "    print(line_new)\n",
    "    print(\"================================================================\")\n",
    "    total_params = 0\n",
    "    total_output = 0\n",
    "    trainable_params = 0\n",
    "    for layer in summary:\n",
    "        # input_shape, output_shape, trainable, nb_params\n",
    "        line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "            layer,\n",
    "            str(summary[layer][\"output_shape\"]),\n",
    "            \"{0:,}\".format(summary[layer][\"nb_params\"]),\n",
    "        )\n",
    "        total_params += summary[layer][\"nb_params\"]\n",
    "        total_output += np.prod(summary[layer][\"output_shape\"])\n",
    "        if \"trainable\" in summary[layer]:\n",
    "            if summary[layer][\"trainable\"] == True:\n",
    "                trainable_params += summary[layer][\"nb_params\"]\n",
    "        print(line_new)\n",
    "    # assume 4 bytes/number (float on cuda).\n",
    "    total_input_size = abs(np.prod([in_size[0][0] for in_size in input_size]) * batch_size * 4. / (1024 ** 2.))\n",
    "    total_output_size = abs(2. * total_output * 4. / (1024 ** 2.))  # x2 for gradients\n",
    "    total_params_size = abs(total_params.numpy() * 4. / (1024 ** 2.))\n",
    "    total_size = total_params_size + total_output_size + total_input_size\n",
    "\n",
    "    print(\"================================================================\")\n",
    "    print(\"Total params: {0:,}\".format(total_params))\n",
    "    print(\"Trainable params: {0:,}\".format(trainable_params))\n",
    "    print(\"Non-trainable params: {0:,}\".format(total_params - trainable_params))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Input size (MB): %0.2f\" % total_input_size)\n",
    "    print(\"Forward/backward pass size (MB): %0.2f\" % total_output_size)\n",
    "    print(\"Params size (MB): %0.2f\" % total_params_size)\n",
    "    print(\"Estimated Total Size (MB): %0.2f\" % total_size)\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    # return summary\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#############Main###############\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "class Run:\n",
    "    def __init__(self, args):\n",
    "        if args['random_seed'] != 0:\n",
    "            random.seed(args['random_seed'])\n",
    "            np.random.seed(args['random_seed'])\n",
    "            torch.manual_seed(args['random_seed'])\n",
    "            torch.cuda.manual_seed_all(args['random_seed'])\n",
    "        \n",
    "        print(\"loading lookups...\")\n",
    "        dicts = self.load_lookups(args)\n",
    "        modelhub = ModelHub(args, dicts)\n",
    "        model = modelhub.pick_model(args, dicts)\n",
    "        print(model)\n",
    "\n",
    "        My_Summary(model,\n",
    "                [(tuple([args['MAX_LENGTH']]),torch.LongTensor), (tuple([len(dicts['ind2c'])]),torch.FloatTensor), (tuple([0]),0), (tuple([args['MAX_ENT_LENGTH']]),torch.LongTensor), (tuple([0]),0)],\n",
    "                device=\"cpu\") #inputs_id, labels, text_inputs, embeddings, tfIdf_inputs\n",
    "\n",
    "        if not args['test_model']:\n",
    "            optimizer = optim.Adam(model.parameters(), weight_decay=args['weight_decay'], lr=args['lr'])\n",
    "            # optimizer = optim.AdamW(model.parameters(), lr=args['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=args['weight_decay'], amsgrad=True)\n",
    "        else:\n",
    "            optimizer = None\n",
    "\n",
    "        if args['tune_wordemb'] == False:\n",
    "            model.freeze_net()\n",
    "        \n",
    "        metrics_hist = defaultdict(lambda: [])\n",
    "        metrics_hist_te = defaultdict(lambda: [])\n",
    "        metrics_hist_tr = defaultdict(lambda: [])\n",
    "\n",
    "        if args['model'].find(\"bert\") != -1:\n",
    "            prepare_instance_func = self.prepare_instance_bert\n",
    "        else:\n",
    "            prepare_instance_func = self.prepare_instance\n",
    "            \n",
    "        train_instances = prepare_instance_func(dicts, args['data_path'], args, args['MAX_LENGTH'])\n",
    "        print(\"train_instances {}\".format(len(train_instances)))\n",
    "        \n",
    "        dev_instances = prepare_instance_func(dicts, args['data_path'].replace('train','dev'), args, args['MAX_LENGTH'])\n",
    "        print(\"dev_instances {}\".format(len(dev_instances)))\n",
    "            \n",
    "        test_instances = prepare_instance_func(dicts, args['data_path'].replace('train','test'), args, args['MAX_LENGTH'])\n",
    "        print(\"test_instances {}\".format(len(test_instances)))\n",
    "        \n",
    "        if args['model'].find(\"bert\") != -1:\n",
    "            collate_func = self.my_collate_bertf\n",
    "        else:\n",
    "            collate_func = self.my_collate\n",
    "        \n",
    "        train_loader = DataLoader(MyDataset(train_instances), args['batch_size'], shuffle=True, collate_fn=collate_func)\n",
    "\n",
    "        dev_loader = DataLoader(MyDataset(dev_instances), 1, shuffle=False, collate_fn=collate_func)\n",
    "  \n",
    "        test_loader = DataLoader(MyDataset(test_instances), 1, shuffle=False, collate_fn=collate_func)\n",
    "        \n",
    "        if not args['test_model'] and args['model'].find(\"bert\") != -1:\n",
    "            #original start\n",
    "            param_optimizer = list(model.named_parameters())\n",
    "            param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
    "            no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "            optimizer_grouped_parameters = [\n",
    "                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                 'weight_decay': 0.01},\n",
    "                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "            ]\n",
    "\n",
    "            num_train_optimization_steps = int(\n",
    "                len(train_instances) / args['batch_size'] + 1) * args['n_epochs']\n",
    "\n",
    "            # optimizer = AdamW(optimizer_grouped_parameters, lr=args['lr'], eps=1e-8)\n",
    "            # optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "            #                      lr=args['lr'],\n",
    "            #                      warmup=0.1,\n",
    "            #                      e=1e-8,\n",
    "            #                      t_total=num_train_optimization_steps)\n",
    "            optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                                 lr=args['lr'],\n",
    "                                 warmup=0.1,\n",
    "                                 t_total=num_train_optimization_steps)\n",
    "            #original end\n",
    "        \n",
    "        scheduler = StepLR(optimizer, step_size=args['step_size'], gamma=args['gamma'])\n",
    "\n",
    "        test_only = args['test_model'] is not None\n",
    "        \n",
    "        train_test = Train_Test()\n",
    "\n",
    "        for epoch in range(args['n_epochs']):\n",
    "\n",
    "            if epoch == 0 and not args['test_model'] and args['save_everything']:\n",
    "                model_dir = os.path.join(args['MODEL_DIR'], '_'.join([args['model'], time.strftime('%b_%d_%H_%M_%S', time.localtime())]))\n",
    "                os.makedirs(model_dir)\n",
    "            elif args['test_model']:\n",
    "                model_dir = os.path.dirname(os.path.abspath(args['test_model']))\n",
    "\n",
    "            if not test_only:\n",
    "                epoch_start = time.time()\n",
    "                losses = train_test.train(args, model, optimizer, scheduler, epoch, args['gpu'], train_loader)\n",
    "                loss = np.mean(losses)\n",
    "                epoch_finish = time.time()\n",
    "                print(\"epoch finish in %.2fs, loss: %.4f\" % (epoch_finish - epoch_start, loss))\n",
    "            else:\n",
    "                loss = np.nan\n",
    "\n",
    "            if epoch == args['n_epochs'] - 1:\n",
    "                print(\"last epoch: testing on dev and test sets\")\n",
    "                test_only = True\n",
    "\n",
    "            # test on dev\n",
    "            evaluation_start = time.time()\n",
    "            metrics = train_test.test(args, model, args['data_path'], \"dev\", args['gpu'], dicts, dev_loader)\n",
    "            evaluation_finish = time.time()\n",
    "            print(\"evaluation finish in %.2fs\" % (evaluation_finish - evaluation_start))\n",
    "            if test_only or epoch == args['n_epochs'] - 1:\n",
    "                metrics_te = train_test.test(args, model, args['data_path'], \"test\", args['gpu'], dicts, test_loader)\n",
    "            else:\n",
    "                metrics_te = defaultdict(float)\n",
    "\n",
    "            if args['use_schedular']:\n",
    "              #Update scheduler\n",
    "              scheduler.step()\n",
    "\n",
    "            metrics_tr = {'loss': loss}\n",
    "            metrics_all = (metrics, metrics_te, metrics_tr)\n",
    "\n",
    "            for name in metrics_all[0].keys():\n",
    "                metrics_hist[name].append(metrics_all[0][name])\n",
    "            for name in metrics_all[1].keys():\n",
    "                metrics_hist_te[name].append(metrics_all[1][name])\n",
    "            for name in metrics_all[2].keys():\n",
    "                metrics_hist_tr[name].append(metrics_all[2][name])\n",
    "            metrics_hist_all = (metrics_hist, metrics_hist_te, metrics_hist_tr)\n",
    "\n",
    "            if args['save_everything']:\n",
    "                self.save_everything(args, metrics_hist_all, model, model_dir, None, args['criterion'], test_only)\n",
    "\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if test_only:\n",
    "                break\n",
    "              \n",
    "            if args['criterion'] in metrics_hist.keys():\n",
    "                if self.early_stop(metrics_hist, args['criterion'], args['patience']):\n",
    "                    #stop training, do tests on test and train sets, and then stop the script\n",
    "                    print(\"%s hasn't improved in %d epochs, early stopping...\" % (args['criterion'], args['patience']))\n",
    "                    test_only = True\n",
    "                    args['test_model'] = '%s/model_best_%s.pth' % (model_dir, args['criterion'])\n",
    "                    model = modelhub.pick_model(args, dicts)\n",
    "                        \n",
    "    def load_lookups(self, args):\n",
    "\n",
    "        csv.field_size_limit(sys.maxsize)\n",
    "        ind2w, w2ind = self.load_vocab_dict(args, args['vocab'])\n",
    "        \n",
    "\n",
    "        #get code and description lookups\n",
    "        if args['Y'] == 'full':\n",
    "            ind2c = self.load_full_codes(args['data_path'], version=args['version'])\n",
    "        else:\n",
    "            codes = set()\n",
    "            with open(\"%sTOP_%s_CODES.csv\" % (args['out_path'], str(args['Y'])), 'r') as labelfile:\n",
    "                lr = csv.reader(labelfile)\n",
    "                for i,row in enumerate(lr):\n",
    "                    codes.add(row[0])\n",
    "            ind2c = {i:c for i,c in enumerate(sorted(codes))}\n",
    "\n",
    "        c2ind = {c:i for i,c in ind2c.items()}\n",
    "\n",
    "        dicts = {'ind2w': ind2w, 'w2ind': w2ind, 'ind2c': ind2c, 'c2ind': c2ind}\n",
    "\n",
    "        return dicts\n",
    "    \n",
    "    def load_vocab_dict(self, args, vocab_file):\n",
    "        vocab = set()\n",
    "\n",
    "        with open(vocab_file, 'r') as vocabfile:\n",
    "            for i, line in enumerate(vocabfile):\n",
    "                line = line.rstrip()\n",
    "                # if line.strip() in vocab:\n",
    "                #     print(line)\n",
    "                if line != '':\n",
    "                    vocab.add(line.strip())\n",
    "\n",
    "        ind2w = {i + 1: w for i, w in enumerate(sorted(vocab))}\n",
    "        w2ind = {w: i for i, w in ind2w.items()}\n",
    "\n",
    "        return ind2w, w2ind\n",
    "    \n",
    "    def load_full_codes(self,train_path, version='mimic3'):\n",
    "        csv.field_size_limit(sys.maxsize)\n",
    "        codes = set()\n",
    "        for split in ['train', 'dev', 'test']:\n",
    "            with open(train_path.replace('train', split), 'r') as f:\n",
    "                lr = csv.reader(f)\n",
    "                next(lr)\n",
    "                for row in lr:\n",
    "                    for code in row[3].split(';'): #codes are in 3rd position after removing hadm_id, 3 when hadm id\n",
    "                        codes.add(code)\n",
    "        codes = set([c for c in codes if c != ''])\n",
    "        ind2c = defaultdict(str, {i:c for i,c in enumerate(sorted(codes))})\n",
    "        return ind2c\n",
    "    \n",
    "    def prepare_instance(self, dicts, filename, args, max_length):\n",
    "      #columns : SUBJECT_ID,\tHADM_ID,\tTEXT,\tLABELS,\tENTITY_ID,\tlength\n",
    "        print(\"reading from file=\",filename)\n",
    "        csv.field_size_limit(sys.maxsize)\n",
    "        ind2w, w2ind, ind2c, c2ind = dicts['ind2w'], dicts['w2ind'], dicts['ind2c'], dicts['c2ind']\n",
    "        instances = []\n",
    "        num_labels = len(dicts['ind2c'])\n",
    "        \n",
    "        if args['use_embd_layer']:\n",
    "            ent2vec = KGEntityToVec().getEntityToVec()\n",
    "            keys_list = list(ent2vec.keys())\n",
    "\n",
    "        if args['use_sentiment']:\n",
    "            tokenizer = tr.AutoTokenizer.from_pretrained(str(args['sentiment_bert']))\n",
    "\n",
    "        if args['use_tfIdf']:\n",
    "            data_to_use = pd.read_csv(filename)\n",
    "          \n",
    "            X_data = data_to_use['TEXT']\n",
    "            X_data = [text.replace('[CLS]','').replace('[SEP]','') for text in X_data]\n",
    "\n",
    "            vectorizer = TfidfVectorizer(max_features=300)\n",
    "            \n",
    "            df_data = vectorizer.fit_transform(X_data)\n",
    "\n",
    "            sequences_data = dict(zip(vectorizer.get_feature_names_out(), df_data.toarray()[0]+1))\n",
    "            \n",
    "            del data_to_use\n",
    "            del X_data\n",
    "            del vectorizer\n",
    "            del df_data\n",
    "\n",
    "        with open(filename, 'r') as infile:\n",
    "            r = csv.reader(infile)\n",
    "            #header\n",
    "            next(r)\n",
    "\n",
    "            count = 0\n",
    "            for row in tqdm(r):\n",
    "\n",
    "                text = row[2] #text is in 2nd column after removing hadm_id, 2 if HADM\n",
    "\n",
    "                labels_idx = np.zeros(num_labels)\n",
    "                labelled = False\n",
    "\n",
    "                for l in row[3].split(';'): #labels are in 3rd column after removing hadm_id, 3 if HADM\n",
    "                    if l in c2ind.keys():\n",
    "                        code = int(c2ind[l])\n",
    "                        labels_idx[code] = 1\n",
    "                        labelled = True\n",
    "                if not labelled:\n",
    "                    continue\n",
    "\n",
    "                tokens_ = text.split()\n",
    "                tokens = []\n",
    "                tokens_id = []\n",
    "                for token in tokens_:\n",
    "                    if token == '[CLS]' or token == '[SEP]':\n",
    "                        continue\n",
    "                    tokens.append(token)\n",
    "                    token_id = w2ind[token] if token in w2ind else len(w2ind) + 1\n",
    "                    tokens_id.append(token_id)\n",
    "\n",
    "                if len(tokens) > max_length:\n",
    "                    tokens = tokens[:max_length]\n",
    "                    tokens_id = tokens_id[:max_length]\n",
    "\n",
    "                if args['use_sentiment']:\n",
    "                    tokens = text.replace('[CLS]', '').replace('[SEP]', '')\n",
    "                    #Bert models can use max 512 tokens\n",
    "                    tokens = tokenizer(tokens,\n",
    "                                       padding='max_length',\n",
    "                                       truncation=True,\n",
    "                                       max_length=512,\n",
    "                                       return_tensors='pt')\n",
    "\n",
    "                if args['use_tfIdf']:\n",
    "                    tf_idf = [sequences_data[token] if token in sequences_data else 1.0 for token in tokens]\n",
    "                else:\n",
    "                    tf_idf = None \n",
    "\n",
    "                if args['use_embd_layer']:\n",
    "                    #getting entity embeddings from KG. Each entity embd is of 200d. Extending to to create a single array.\n",
    "                    entities = row[4] #entities are stored in 4th column\n",
    "                    entities_ = entities.split()\n",
    "                    ent_found = False\n",
    "\n",
    "                    #for use in embedding layer\n",
    "                    entities_id = set()\n",
    "                    for entity in entities_[:args['MAX_ENT_LENGTH']]:\n",
    "                        ent_id = keys_list.index(entity)+1 if entity in keys_list else len(keys_list) + 1\n",
    "                        entities_id.add(ent_id)\n",
    "                        ent_found = True\n",
    "                    \n",
    "                    if not ent_found:\n",
    "                        continue\n",
    "\n",
    "                    entity_embeddings = list(entities_id)\n",
    "                else:\n",
    "                    entity_embeddings = None       \n",
    "\n",
    "                dict_instance = {'label': labels_idx,\n",
    "                                     'tokens': tokens,\n",
    "                                     \"entity_embd\":entity_embeddings,\n",
    "                                     \"tokens_id\": tokens_id,\n",
    "                                      \"tf_idf\": tf_idf\n",
    "                                 }\n",
    "\n",
    "                instances.append(dict_instance)\n",
    "\n",
    "                count += 1\n",
    "            \n",
    "                if args['instance_count'] != 'full' and count == int(args['instance_count']):\n",
    "                    break\n",
    "\n",
    "        return instances\n",
    "    \n",
    "    def prepare_instance_bert(self, dicts, filename, args, max_length):\n",
    "      #columns : SUBJECT_ID,\tHADM_ID,\tTEXT,\tLABELS,\tENTITY_ID,\tlength\n",
    "        csv.field_size_limit(sys.maxsize)\n",
    "        ind2w, w2ind, ind2c, c2ind = dicts['ind2w'], dicts['w2ind'], dicts['ind2c'], dicts['c2ind']\n",
    "        instances = []\n",
    "        num_labels = len(dicts['ind2c'])\n",
    "        \n",
    "        wp_tokenizer = tr.BertTokenizer.from_pretrained(args['pretrained_bert'], do_lower_case=True)\n",
    "\n",
    "        ent2vec = KGEntityToVec().getEntityToVec()\n",
    "        \n",
    "        if args['use_embd_layer']:\n",
    "            keys_list = list(ent2vec.keys())\n",
    "\n",
    "        with open(filename, 'r') as infile:\n",
    "            r = csv.reader(infile)\n",
    "            #header\n",
    "            next(r)\n",
    "            count = 0\n",
    "            for row in tqdm(r):\n",
    "                \n",
    "                text = row[2] #text is in 2nd column now after removing hadm_id, if HADM_ID then text is in 3rd column\n",
    "\n",
    "                labels_idx = np.zeros(num_labels)\n",
    "                labelled = False\n",
    "\n",
    "                for l in row[3].split(';'): #labels are in 3rd column after removing hadm_id\n",
    "                    if l in c2ind.keys():\n",
    "                        code = int(c2ind[l])\n",
    "                        labels_idx[code] = 1\n",
    "                        labelled = True\n",
    "                if not labelled:\n",
    "                    continue\n",
    "\n",
    "                # original 2 start\n",
    "                ##Changes made by prantik for obove code start\n",
    "                tokens = wp_tokenizer.tokenize(text)\n",
    "                tokens = list(filter(lambda a: (a != \"[CLS]\" and a != \"[SEP]\"), tokens))[0:max_length-2]\n",
    "                tokens.insert(0, '[CLS]')\n",
    "                tokens.append('[SEP]')\n",
    "                ##Changes made by prantik for obove code end\n",
    "\n",
    "                tokens_id = wp_tokenizer.convert_tokens_to_ids(tokens)\n",
    "                masks = [1] * len(tokens)\n",
    "                segments = [0] * len(tokens)\n",
    "                # original 2 end\n",
    "\n",
    "                \n",
    "                #getting entity embeddings from KG. Each entity embd is of 200d. Extending to to create a single array.\n",
    "                entities = row[4] #entities are stored in 4th column\n",
    "                entities_ = entities.split()\n",
    "                ent_found = False\n",
    "                \n",
    "                if args['use_embd_layer']:\n",
    "                    #for use in embedding layer\n",
    "                    entities_id = set()\n",
    "                    for entity in entities_[:args['MAX_ENT_LENGTH']]:\n",
    "                        ent_id = keys_list.index(entity)+1 if entity in keys_list else len(keys_list) + 1\n",
    "                        entities_id.add(ent_id)\n",
    "                        ent_found = True\n",
    "                    \n",
    "                    if not ent_found:\n",
    "                        continue\n",
    "\n",
    "                    entity_embeddings = list(entities_id)\n",
    "                else:\n",
    "                    entity_embeddings = None\n",
    "    \n",
    "                dict_instance = {'label':labels_idx, 'tokens':tokens, \"entity_embd\":entity_embeddings,\n",
    "                                 \"tokens_id\":tokens_id, \"segments\":segments, \"masks\":masks}\n",
    "\n",
    "                instances.append(dict_instance)\n",
    "                \n",
    "                count += 1\n",
    "            \n",
    "                if args['instance_count'] != 'full' and count == int(args['instance_count']):\n",
    "                    break\n",
    "\n",
    "        return instances\n",
    "\n",
    "\n",
    "    def my_collate(self, x):\n",
    "        words = [x_['tokens_id'] for x_ in x]\n",
    "        max_seq_len = max([len(w) for w in words])\n",
    "        if max_seq_len < args['MAX_LENGTH']:\n",
    "            max_seq_len = args['MAX_LENGTH']\n",
    "\n",
    "        inputs_id = self.pad_sequence(words, max_seq_len)\n",
    "\n",
    "        labels = [x_['label'] for x_ in x]\n",
    "\n",
    "        if args['use_sentiment']:\n",
    "            text_inputs = [[x_['tokens']['input_ids'], x_['tokens']['attention_mask']] for x_ in x]\n",
    "        else:\n",
    "            text_inputs = []\n",
    "\n",
    "        embeddings = None\n",
    "        if args['use_embd_layer']:\n",
    "            embeddings = [x_['entity_embd'] for x_ in x]\n",
    "            emb_list = [len(x) for x in embeddings]\n",
    "            max_embd_len = max(emb_list)\n",
    "            if max_embd_len < args['MAX_ENT_LENGTH']:\n",
    "                max_embd_len = args['MAX_ENT_LENGTH']\n",
    "            embeddings = self.pad_sequence(embeddings, max_embd_len)\n",
    "        \n",
    "        tfIdf_inputs = None\n",
    "        if args['use_tfIdf']:\n",
    "            tfIdf_inputs = [x_['tf_idf'] for x_ in x]\n",
    "            tfIdf_inputs = self.pad_sequence(tfIdf_inputs, max_seq_len, np.float)\n",
    "\n",
    "        return inputs_id, labels, text_inputs, embeddings, tfIdf_inputs\n",
    "\n",
    "    def my_collate_bert(self, x):\n",
    "        words = [x_['tokens_id'] for x_ in x]\n",
    "        segments = [x_['segments'] for x_ in x]\n",
    "        masks = [x_['masks'] for x_ in x]\n",
    "        embeddings = [x_['entity_embd'] for x_ in x]\n",
    "        \n",
    "        seq_len = [len(w) for w in words]\n",
    "        max_seq_len = max(seq_len)\n",
    "        \n",
    "        if args['use_embd_layer']:\n",
    "            #for embedding layer\n",
    "            max_embd_len = max([len(x) for x in embeddings])\n",
    "            if max_embd_len < args['MAX_ENT_LENGTH']:\n",
    "                max_embd_len = args['MAX_ENT_LENGTH']\n",
    "        try:\n",
    "            inputs_id = self.pad_sequence(words, max_seq_len)\n",
    "            segments = self.pad_sequence(segments, max_seq_len)\n",
    "            masks = self.pad_sequence(masks, max_seq_len)\n",
    "            if args['use_embd_layer']:\n",
    "                #for embedding layer\n",
    "                embeddings = self.pad_sequence(embeddings, max_embd_len)\n",
    "        except:\n",
    "            print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "            raise\n",
    "\n",
    "        labels = [x_['label'] for x_ in x]\n",
    "\n",
    "        return inputs_id, segments, masks, embeddings, labels\n",
    "    \n",
    "    \n",
    "    def pad_sequence(self, x, max_len, type=np.int):\n",
    "\n",
    "        padded_x = np.zeros((len(x), max_len), dtype=type)\n",
    "        for i, row in enumerate(x):\n",
    "            if max_len >= len(row):\n",
    "                padded_x[i][:len(row)] = row\n",
    "            else:\n",
    "                padded_x[i][:max_len] = row[:max_len] #trancate\n",
    "\n",
    "        return padded_x\n",
    "    \n",
    "    def save_metrics(self, metrics_hist_all, model_dir):\n",
    "        with open(model_dir + \"/metrics.json\", 'w') as metrics_file:\n",
    "            #concatenate dev, train metrics into one dict\n",
    "            data = metrics_hist_all[0].copy()\n",
    "            data.update({\"%s_te\" % (name):val for (name,val) in metrics_hist_all[1].items()})\n",
    "            data.update({\"%s_tr\" % (name):val for (name,val) in metrics_hist_all[2].items()})\n",
    "            json.dump(data, metrics_file, indent=1)\n",
    "            \n",
    "    def save_everything(self, args, metrics_hist_all, model, model_dir, params, criterion, evaluate=False):\n",
    "\n",
    "        self.save_args(args, model_dir)\n",
    "\n",
    "        self.save_metrics(metrics_hist_all, model_dir)\n",
    "\n",
    "        if not evaluate:\n",
    "            #save the model with the best criterion metric\n",
    "            if not np.all(np.isnan(metrics_hist_all[0][criterion])):\n",
    "                if criterion == 'loss_dev':\n",
    "                    eval_val = np.nanargmin(metrics_hist_all[0][criterion])\n",
    "                else:\n",
    "                    eval_val = np.nanargmax(metrics_hist_all[0][criterion])\n",
    "\n",
    "                if eval_val == len(metrics_hist_all[0][criterion]) - 1:\n",
    "                    print(\"saving model==\")\n",
    "                    sd = model.cpu().state_dict()\n",
    "                    torch.save(sd, model_dir + \"/model_best_%s.pth\" % criterion)\n",
    "                    if args['gpu'] >= 0:\n",
    "                        model.cuda(args['gpu'])\n",
    "        print(\"saved metrics, params, model to directory %s\\n\" % (model_dir))\n",
    "\n",
    "    def save_args(self, args, model_path):\n",
    "        file_path = model_path + \"/args.json\"\n",
    "        if not os.path.exists(file_path):\n",
    "            with open(file_path, 'w') as args_file:\n",
    "                json.dump(args, args_file)\n",
    "        \n",
    "    def early_stop(self, metrics_hist, criterion, patience):\n",
    "        if not np.all(np.isnan(metrics_hist[criterion])):\n",
    "            if len(metrics_hist[criterion]) >= patience:\n",
    "                if criterion == 'loss_dev':\n",
    "                    return np.nanargmin(metrics_hist[criterion]) < len(metrics_hist[criterion]) - patience\n",
    "                else:\n",
    "                    return np.nanargmax(metrics_hist[criterion]) < len(metrics_hist[criterion]) - patience\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set proper path values in args{} and hit for data processig and saving.\n",
    "DataProcessing(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLu1OLM0gtqe",
    "outputId": "eb289489-e36e-460d-cea3-fd803c32fcd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lookups...\n",
      "loading pretrained embeddings from /content/drive/MyDrive/Thesis/DeepDifferentialDiagnosis/data/25.10.2021-Old-Compare/test/processed_full.embed\n",
      "adding unk embedding\n",
      "loading pretrained embeddings from /content/drive/MyDrive/Thesis/DeepDifferentialDiagnosis/data/25.10.2021-Old-Compare/test/processed_full.embed\n",
      "adding unk embedding\n",
      "KG_MultiResCNN(\n",
      "  (word_rep): WordRep(\n",
      "    (embed): Embedding(48497, 100, padding_idx=0)\n",
      "    (embed_drop): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (kg_embd): EntityEmbedding(\n",
      "    (embed): Embedding(17815, 200)\n",
      "    (embed_drop): Dropout(p=0.2, inplace=False)\n",
      "    (dim_red): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (feature_red): Linear(in_features=30, out_features=3000, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (conv): MRCNNLayer(\n",
      "    (conv): ModuleList(\n",
      "      (channel-3): ModuleList(\n",
      "        (baseconv): Conv1d(100, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (resconv-0): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(100, 100, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "            (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (resconv-1): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(50, 50, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "            (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (channel-5): ModuleList(\n",
      "        (baseconv): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (resconv-0): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "            (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (resconv-1): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(50, 50, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "            (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (channel-7): ModuleList(\n",
      "        (baseconv): Conv1d(100, 100, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (resconv-0): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(100, 100, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "            (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (resconv-1): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(50, 50, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "            (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (channel-9): ModuleList(\n",
      "        (baseconv): Conv1d(100, 100, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "        (resconv-0): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(100, 100, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "            (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (resconv-1): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(50, 50, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "            (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (channel-13): ModuleList(\n",
      "        (baseconv): Conv1d(100, 100, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "        (resconv-0): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(13,), stride=(1,), padding=(6,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(100, 100, kernel_size=(13,), stride=(1,), padding=(6,), bias=False)\n",
      "            (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (resconv-1): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(13,), stride=(1,), padding=(6,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(50, 50, kernel_size=(13,), stride=(1,), padding=(6,), bias=False)\n",
      "            (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (channel-15): ModuleList(\n",
      "        (baseconv): Conv1d(100, 100, kernel_size=(15,), stride=(1,), padding=(7,))\n",
      "        (resconv-0): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(15,), stride=(1,), padding=(7,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(100, 100, kernel_size=(15,), stride=(1,), padding=(7,), bias=False)\n",
      "            (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (resconv-1): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(15,), stride=(1,), padding=(7,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(50, 50, kernel_size=(15,), stride=(1,), padding=(7,), bias=False)\n",
      "            (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (channel-17): ModuleList(\n",
      "        (baseconv): Conv1d(100, 100, kernel_size=(17,), stride=(1,), padding=(8,))\n",
      "        (resconv-0): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(100, 100, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
      "            (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (resconv-1): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(50, 50, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
      "            (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (channel-23): ModuleList(\n",
      "        (baseconv): Conv1d(100, 100, kernel_size=(23,), stride=(1,), padding=(11,))\n",
      "        (resconv-0): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(23,), stride=(1,), padding=(11,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(100, 100, kernel_size=(23,), stride=(1,), padding=(11,), bias=False)\n",
      "            (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (resconv-1): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(23,), stride=(1,), padding=(11,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(50, 50, kernel_size=(23,), stride=(1,), padding=(11,), bias=False)\n",
      "            (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (channel-29): ModuleList(\n",
      "        (baseconv): Conv1d(100, 100, kernel_size=(29,), stride=(1,), padding=(14,))\n",
      "        (resconv-0): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(29,), stride=(1,), padding=(14,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(100, 100, kernel_size=(29,), stride=(1,), padding=(14,), bias=False)\n",
      "            (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 100, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (resconv-1): ResidualBlock(\n",
      "          (left): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(29,), stride=(1,), padding=(14,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Tanh()\n",
      "            (3): Conv1d(50, 50, kernel_size=(29,), stride=(1,), padding=(14,), bias=False)\n",
      "            (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (shortcut): Sequential(\n",
      "            (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): OutputLayer(\n",
      "    (U): Linear(in_features=450, out_features=41125, bias=True)\n",
      "    (final): Linear(in_features=450, out_features=41125, bias=True)\n",
      "    (loss_function): BCEWithLogitsLoss()\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1            [-1, 3000, 100]       4,849,700\n",
      "           Dropout-2            [-1, 3000, 100]               0\n",
      "           WordRep-3            [-1, 3000, 100]               0\n",
      "         Embedding-4              [-1, 30, 200]       3,563,000\n",
      "           Dropout-5              [-1, 30, 200]               0\n",
      "   EntityEmbedding-6              [-1, 30, 200]               0\n",
      "            Linear-7              [-1, 30, 100]          20,100\n",
      "           Dropout-8            [-1, 3030, 100]               0\n",
      "            Conv1d-9            [-1, 100, 3030]          30,100\n",
      "           Conv1d-10            [-1, 100, 3030]          30,000\n",
      "      BatchNorm1d-11            [-1, 100, 3030]             200\n",
      "             Tanh-12            [-1, 100, 3030]               0\n",
      "           Conv1d-13            [-1, 100, 3030]          30,000\n",
      "      BatchNorm1d-14            [-1, 100, 3030]             200\n",
      "           Conv1d-15            [-1, 100, 3030]          10,000\n",
      "      BatchNorm1d-16            [-1, 100, 3030]             200\n",
      "          Dropout-17            [-1, 100, 3030]               0\n",
      "    ResidualBlock-18            [-1, 100, 3030]               0\n",
      "           Conv1d-19             [-1, 50, 3030]          15,000\n",
      "      BatchNorm1d-20             [-1, 50, 3030]             100\n",
      "             Tanh-21             [-1, 50, 3030]               0\n",
      "           Conv1d-22             [-1, 50, 3030]           7,500\n",
      "      BatchNorm1d-23             [-1, 50, 3030]             100\n",
      "           Conv1d-24             [-1, 50, 3030]           5,000\n",
      "      BatchNorm1d-25             [-1, 50, 3030]             100\n",
      "          Dropout-26             [-1, 50, 3030]               0\n",
      "    ResidualBlock-27             [-1, 50, 3030]               0\n",
      "           Conv1d-28            [-1, 100, 3030]          50,100\n",
      "           Conv1d-29            [-1, 100, 3030]          50,000\n",
      "      BatchNorm1d-30            [-1, 100, 3030]             200\n",
      "             Tanh-31            [-1, 100, 3030]               0\n",
      "           Conv1d-32            [-1, 100, 3030]          50,000\n",
      "      BatchNorm1d-33            [-1, 100, 3030]             200\n",
      "           Conv1d-34            [-1, 100, 3030]          10,000\n",
      "      BatchNorm1d-35            [-1, 100, 3030]             200\n",
      "          Dropout-36            [-1, 100, 3030]               0\n",
      "    ResidualBlock-37            [-1, 100, 3030]               0\n",
      "           Conv1d-38             [-1, 50, 3030]          25,000\n",
      "      BatchNorm1d-39             [-1, 50, 3030]             100\n",
      "             Tanh-40             [-1, 50, 3030]               0\n",
      "           Conv1d-41             [-1, 50, 3030]          12,500\n",
      "      BatchNorm1d-42             [-1, 50, 3030]             100\n",
      "           Conv1d-43             [-1, 50, 3030]           5,000\n",
      "      BatchNorm1d-44             [-1, 50, 3030]             100\n",
      "          Dropout-45             [-1, 50, 3030]               0\n",
      "    ResidualBlock-46             [-1, 50, 3030]               0\n",
      "           Conv1d-47            [-1, 100, 3030]          70,100\n",
      "           Conv1d-48            [-1, 100, 3030]          70,000\n",
      "      BatchNorm1d-49            [-1, 100, 3030]             200\n",
      "             Tanh-50            [-1, 100, 3030]               0\n",
      "           Conv1d-51            [-1, 100, 3030]          70,000\n",
      "      BatchNorm1d-52            [-1, 100, 3030]             200\n",
      "           Conv1d-53            [-1, 100, 3030]          10,000\n",
      "      BatchNorm1d-54            [-1, 100, 3030]             200\n",
      "          Dropout-55            [-1, 100, 3030]               0\n",
      "    ResidualBlock-56            [-1, 100, 3030]               0\n",
      "           Conv1d-57             [-1, 50, 3030]          35,000\n",
      "      BatchNorm1d-58             [-1, 50, 3030]             100\n",
      "             Tanh-59             [-1, 50, 3030]               0\n",
      "           Conv1d-60             [-1, 50, 3030]          17,500\n",
      "      BatchNorm1d-61             [-1, 50, 3030]             100\n",
      "           Conv1d-62             [-1, 50, 3030]           5,000\n",
      "      BatchNorm1d-63             [-1, 50, 3030]             100\n",
      "          Dropout-64             [-1, 50, 3030]               0\n",
      "    ResidualBlock-65             [-1, 50, 3030]               0\n",
      "           Conv1d-66            [-1, 100, 3030]          90,100\n",
      "           Conv1d-67            [-1, 100, 3030]          90,000\n",
      "      BatchNorm1d-68            [-1, 100, 3030]             200\n",
      "             Tanh-69            [-1, 100, 3030]               0\n",
      "           Conv1d-70            [-1, 100, 3030]          90,000\n",
      "      BatchNorm1d-71            [-1, 100, 3030]             200\n",
      "           Conv1d-72            [-1, 100, 3030]          10,000\n",
      "      BatchNorm1d-73            [-1, 100, 3030]             200\n",
      "          Dropout-74            [-1, 100, 3030]               0\n",
      "    ResidualBlock-75            [-1, 100, 3030]               0\n",
      "           Conv1d-76             [-1, 50, 3030]          45,000\n",
      "      BatchNorm1d-77             [-1, 50, 3030]             100\n",
      "             Tanh-78             [-1, 50, 3030]               0\n",
      "           Conv1d-79             [-1, 50, 3030]          22,500\n",
      "      BatchNorm1d-80             [-1, 50, 3030]             100\n",
      "           Conv1d-81             [-1, 50, 3030]           5,000\n",
      "      BatchNorm1d-82             [-1, 50, 3030]             100\n",
      "          Dropout-83             [-1, 50, 3030]               0\n",
      "    ResidualBlock-84             [-1, 50, 3030]               0\n",
      "           Conv1d-85            [-1, 100, 3030]         130,100\n",
      "           Conv1d-86            [-1, 100, 3030]         130,000\n",
      "      BatchNorm1d-87            [-1, 100, 3030]             200\n",
      "             Tanh-88            [-1, 100, 3030]               0\n",
      "           Conv1d-89            [-1, 100, 3030]         130,000\n",
      "      BatchNorm1d-90            [-1, 100, 3030]             200\n",
      "           Conv1d-91            [-1, 100, 3030]          10,000\n",
      "      BatchNorm1d-92            [-1, 100, 3030]             200\n",
      "          Dropout-93            [-1, 100, 3030]               0\n",
      "    ResidualBlock-94            [-1, 100, 3030]               0\n",
      "           Conv1d-95             [-1, 50, 3030]          65,000\n",
      "      BatchNorm1d-96             [-1, 50, 3030]             100\n",
      "             Tanh-97             [-1, 50, 3030]               0\n",
      "           Conv1d-98             [-1, 50, 3030]          32,500\n",
      "      BatchNorm1d-99             [-1, 50, 3030]             100\n",
      "          Conv1d-100             [-1, 50, 3030]           5,000\n",
      "     BatchNorm1d-101             [-1, 50, 3030]             100\n",
      "         Dropout-102             [-1, 50, 3030]               0\n",
      "   ResidualBlock-103             [-1, 50, 3030]               0\n",
      "          Conv1d-104            [-1, 100, 3030]         150,100\n",
      "          Conv1d-105            [-1, 100, 3030]         150,000\n",
      "     BatchNorm1d-106            [-1, 100, 3030]             200\n",
      "            Tanh-107            [-1, 100, 3030]               0\n",
      "          Conv1d-108            [-1, 100, 3030]         150,000\n",
      "     BatchNorm1d-109            [-1, 100, 3030]             200\n",
      "          Conv1d-110            [-1, 100, 3030]          10,000\n",
      "     BatchNorm1d-111            [-1, 100, 3030]             200\n",
      "         Dropout-112            [-1, 100, 3030]               0\n",
      "   ResidualBlock-113            [-1, 100, 3030]               0\n",
      "          Conv1d-114             [-1, 50, 3030]          75,000\n",
      "     BatchNorm1d-115             [-1, 50, 3030]             100\n",
      "            Tanh-116             [-1, 50, 3030]               0\n",
      "          Conv1d-117             [-1, 50, 3030]          37,500\n",
      "     BatchNorm1d-118             [-1, 50, 3030]             100\n",
      "          Conv1d-119             [-1, 50, 3030]           5,000\n",
      "     BatchNorm1d-120             [-1, 50, 3030]             100\n",
      "         Dropout-121             [-1, 50, 3030]               0\n",
      "   ResidualBlock-122             [-1, 50, 3030]               0\n",
      "          Conv1d-123            [-1, 100, 3030]         170,100\n",
      "          Conv1d-124            [-1, 100, 3030]         170,000\n",
      "     BatchNorm1d-125            [-1, 100, 3030]             200\n",
      "            Tanh-126            [-1, 100, 3030]               0\n",
      "          Conv1d-127            [-1, 100, 3030]         170,000\n",
      "     BatchNorm1d-128            [-1, 100, 3030]             200\n",
      "          Conv1d-129            [-1, 100, 3030]          10,000\n",
      "     BatchNorm1d-130            [-1, 100, 3030]             200\n",
      "         Dropout-131            [-1, 100, 3030]               0\n",
      "   ResidualBlock-132            [-1, 100, 3030]               0\n",
      "          Conv1d-133             [-1, 50, 3030]          85,000\n",
      "     BatchNorm1d-134             [-1, 50, 3030]             100\n",
      "            Tanh-135             [-1, 50, 3030]               0\n",
      "          Conv1d-136             [-1, 50, 3030]          42,500\n",
      "     BatchNorm1d-137             [-1, 50, 3030]             100\n",
      "          Conv1d-138             [-1, 50, 3030]           5,000\n",
      "     BatchNorm1d-139             [-1, 50, 3030]             100\n",
      "         Dropout-140             [-1, 50, 3030]               0\n",
      "   ResidualBlock-141             [-1, 50, 3030]               0\n",
      "          Conv1d-142            [-1, 100, 3030]         230,100\n",
      "          Conv1d-143            [-1, 100, 3030]         230,000\n",
      "     BatchNorm1d-144            [-1, 100, 3030]             200\n",
      "            Tanh-145            [-1, 100, 3030]               0\n",
      "          Conv1d-146            [-1, 100, 3030]         230,000\n",
      "     BatchNorm1d-147            [-1, 100, 3030]             200\n",
      "          Conv1d-148            [-1, 100, 3030]          10,000\n",
      "     BatchNorm1d-149            [-1, 100, 3030]             200\n",
      "         Dropout-150            [-1, 100, 3030]               0\n",
      "   ResidualBlock-151            [-1, 100, 3030]               0\n",
      "          Conv1d-152             [-1, 50, 3030]         115,000\n",
      "     BatchNorm1d-153             [-1, 50, 3030]             100\n",
      "            Tanh-154             [-1, 50, 3030]               0\n",
      "          Conv1d-155             [-1, 50, 3030]          57,500\n",
      "     BatchNorm1d-156             [-1, 50, 3030]             100\n",
      "          Conv1d-157             [-1, 50, 3030]           5,000\n",
      "     BatchNorm1d-158             [-1, 50, 3030]             100\n",
      "         Dropout-159             [-1, 50, 3030]               0\n",
      "   ResidualBlock-160             [-1, 50, 3030]               0\n",
      "          Conv1d-161            [-1, 100, 3030]         290,100\n",
      "          Conv1d-162            [-1, 100, 3030]         290,000\n",
      "     BatchNorm1d-163            [-1, 100, 3030]             200\n",
      "            Tanh-164            [-1, 100, 3030]               0\n",
      "          Conv1d-165            [-1, 100, 3030]         290,000\n",
      "     BatchNorm1d-166            [-1, 100, 3030]             200\n",
      "          Conv1d-167            [-1, 100, 3030]          10,000\n",
      "     BatchNorm1d-168            [-1, 100, 3030]             200\n",
      "         Dropout-169            [-1, 100, 3030]               0\n",
      "   ResidualBlock-170            [-1, 100, 3030]               0\n",
      "          Conv1d-171             [-1, 50, 3030]         145,000\n",
      "     BatchNorm1d-172             [-1, 50, 3030]             100\n",
      "            Tanh-173             [-1, 50, 3030]               0\n",
      "          Conv1d-174             [-1, 50, 3030]          72,500\n",
      "     BatchNorm1d-175             [-1, 50, 3030]             100\n",
      "          Conv1d-176             [-1, 50, 3030]           5,000\n",
      "     BatchNorm1d-177             [-1, 50, 3030]             100\n",
      "         Dropout-178             [-1, 50, 3030]               0\n",
      "   ResidualBlock-179             [-1, 50, 3030]               0\n",
      "      MRCNNLayer-180            [-1, 3030, 450]               0\n",
      "BCEWithLogitsLoss-181                       [-1]               0\n",
      "     OutputLayer-182                [-1, 41125]               0\n",
      "================================================================\n",
      "Total params: 13,114,300\n",
      "Trainable params: 13,114,300\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 321.73\n",
      "Params size (MB): 50.03\n",
      "Estimated Total Size (MB): 371.76\n",
      "----------------------------------------------------------------\n",
      "reading from file= /content/drive/MyDrive/Thesis/DeepDifferentialDiagnosis/data/25.10.2021-Old-Compare/test/train_full.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28789it [00:23, 1202.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_instances 28787\n",
      "reading from file= /content/drive/MyDrive/Thesis/DeepDifferentialDiagnosis/data/25.10.2021-Old-Compare/test/dev_full.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4082it [00:03, 1300.26it/s]"
     ]
    }
   ],
   "source": [
    "#Set proper values in args{} and hit for training, validating and testing.\n",
    "Run(args)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Final_KG_MCNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
